{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a175a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def clean_prediction(pred_text: str):\n",
    "#     if pred_text is None:\n",
    "#         return \"\"\n",
    "\n",
    "#     # láº¥y pháº§n sau \"assistant\"\n",
    "#     if \"assistant\" in pred_text:\n",
    "#         pred_text = pred_text.split(\"assistant\", 1)[-1]\n",
    "\n",
    "#     # strip whitespace\n",
    "#     return pred_text.strip()\n",
    "\n",
    "# def extract_number(text):\n",
    "#     nums = re.findall(r\"-?\\d+\\.?\\d*\", text)\n",
    "#     if len(nums) == 0:\n",
    "#         return None\n",
    "#     try:\n",
    "#         return float(nums[0])\n",
    "#     except:\n",
    "#         return None\n",
    "\n",
    "# def get_answer_type(gt: str):\n",
    "#     gt = gt.strip().lower()\n",
    "\n",
    "#     if gt in [\"yes\", \"no\"]:\n",
    "#         return \"boolean\"\n",
    "\n",
    "#     try:\n",
    "#         float(gt)\n",
    "#         return \"numeric\"\n",
    "#     except:\n",
    "#         return \"string\"\n",
    "\n",
    "# def evaluate_sample(pred_text, gt):\n",
    "#     pred_text = clean_prediction(pred_text)\n",
    "#     answer_type = get_answer_type(gt)\n",
    "\n",
    "#     # YES / NO\n",
    "#     if answer_type == \"boolean\":\n",
    "#         pred = pred_text.lower()\n",
    "#         if \"yes\" in pred:\n",
    "#             return gt.lower() == \"yes\"\n",
    "#         if \"no\" in pred:\n",
    "#             return gt.lower() == \"no\"\n",
    "#         return False\n",
    "\n",
    "#     # NUMERIC\n",
    "#     if answer_type == \"numeric\":\n",
    "#         pred_num = extract_number(pred_text)\n",
    "#         gt_num = float(gt)\n",
    "\n",
    "#         if pred_num is None:\n",
    "#             return False\n",
    "\n",
    "#         # exact match\n",
    "#         if pred_num == gt_num:\n",
    "#             return True\n",
    "\n",
    "#         # relaxed (5%)\n",
    "#         if abs(pred_num - gt_num) / max(1.0, abs(gt_num)) <= 0.05:\n",
    "#             return True\n",
    "\n",
    "#         return False\n",
    "\n",
    "#     # STRING\n",
    "#     pred = pred_text.lower()\n",
    "#     gt = gt.lower()\n",
    "\n",
    "#     return gt in pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27edde60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# def load_jsonl(path):\n",
    "#     records = []\n",
    "#     with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if not line:\n",
    "#                 continue\n",
    "#             records.append(json.loads(line))\n",
    "#     return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60eaf436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_record(record):\n",
    "#     pred_text = record[\"pred_text\"]\n",
    "#     gts = record[\"gts\"]  # list\n",
    "\n",
    "#     for gt in gts:\n",
    "#         if evaluate_sample(pred_text, gt):\n",
    "#             return True\n",
    "#     return False\n",
    "# def benchmark_jsonl(path):\n",
    "#     records = load_jsonl(path)\n",
    "\n",
    "#     total = 0\n",
    "#     correct = 0\n",
    "\n",
    "#     # optional: breakdown theo loáº¡i\n",
    "#     stats = {\n",
    "#         \"boolean\": {\"correct\": 0, \"total\": 0},\n",
    "#         \"numeric\": {\"correct\": 0, \"total\": 0},\n",
    "#         \"string\": {\"correct\": 0, \"total\": 0},\n",
    "#     }\n",
    "\n",
    "#     for rec in records:\n",
    "#         total += 1\n",
    "#         is_correct = evaluate_record(rec)\n",
    "#         if is_correct:\n",
    "#             correct += 1\n",
    "\n",
    "#         # thá»‘ng kÃª theo loáº¡i answer (dá»±a trÃªn GT Ä‘áº§u tiÃªn)\n",
    "#         gt0 = rec[\"gts\"][0]\n",
    "#         ans_type = get_answer_type(gt0)\n",
    "#         stats[ans_type][\"total\"] += 1\n",
    "#         if is_correct:\n",
    "#             stats[ans_type][\"correct\"] += 1\n",
    "\n",
    "#     acc = correct / total if total > 0 else 0.0\n",
    "#     return acc, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be1e910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc, stats = benchmark_jsonl(\"D:\\\\AI\\\\KLTN\\\\chartqa_qwenvl_test_predictions.jsonl\")\n",
    "\n",
    "# print(f\"Overall Accuracy: {acc:.4f}\")\n",
    "\n",
    "# for k, v in stats.items():\n",
    "#     if v[\"total\"] > 0:\n",
    "#         print(f\"{k}: {v['correct']}/{v['total']} = {v['correct']/v['total']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d8797e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json, csv, os, sys\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def load_jsonl(path):\n",
    "    records = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "def distance(x1, x2):\n",
    "    return min(1, abs((x1 - x2) / (x1+1e-15)))\n",
    "\n",
    "\n",
    "def compute_cost_matrix(a1, a2):\n",
    "    cost_matrix = np.zeros((len(a1), len(a2)))\n",
    "    for index1, elt1 in enumerate(a1):\n",
    "        for index2, elt2 in enumerate(a2):\n",
    "            cost_matrix[index1, index2] = distance(elt1, elt2)\n",
    "    return cost_matrix\n",
    "\n",
    "\n",
    "def compute_score(lst1, lst2):\n",
    "    cost_matrix = compute_cost_matrix(lst1, lst2)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    cost = cost_matrix[row_ind, col_ind].sum()\n",
    "    score = 1 - cost / max(len(lst1), len(lst2))\n",
    "    return score\n",
    "\n",
    "def remove_strings(lst):\n",
    "    new_lst = []\n",
    "    for elt in lst:\n",
    "        elt = str(elt).replace(\"%\", '')\n",
    "        # Filter out strings.\n",
    "        try:\n",
    "            new_lst.append(float(elt))\n",
    "        except:\n",
    "            continue\n",
    "    return new_lst\n",
    "\n",
    "def benchmark(path):\n",
    "    records = load_jsonl(path)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    for record in records:\n",
    "        # Ground truth numbers (list)\n",
    "        gts = record[\"gts\"]\n",
    "        flattened = remove_strings(gts)\n",
    "\n",
    "        # Prediction numbers (list or single)\n",
    "        pred_nums = record[\"pred_num\"]\n",
    "\n",
    "        if not isinstance(pred_nums, list):\n",
    "            pred_nums = [pred_nums]\n",
    "\n",
    "        flattened2 = remove_strings(pred_nums)\n",
    "\n",
    "        # If no numeric prediction â†’ fail\n",
    "        if len(flattened2) == 0 or len(flattened) == 0:\n",
    "            score = 0.0\n",
    "        else:\n",
    "            score = compute_score(flattened, flattened2)\n",
    "\n",
    "        scores.append(score)\n",
    "\n",
    "    accuracy = sum(scores) / len(scores) if scores else 0.0\n",
    "    # print(\"Accuracy:\", accuracy)\n",
    "    return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7945fcd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy: 0.2012687740145013\n"
     ]
    }
   ],
   "source": [
    "scores = benchmark(\"D:\\\\AI\\\\KLTN\\\\chartqa_qwenvl_test_predictions.jsonl\")\n",
    "print(\"Final Accuracy:\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678627d2",
   "metadata": {},
   "source": [
    "## origin evaluation of chartQA-PRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c969bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ast\n",
    "# import re\n",
    "# from typing import List, Optional, Any, Tuple, Dict\n",
    "# import pandas as pd\n",
    "# from anls import anls_score\n",
    "# import json\n",
    "# import argparse\n",
    "\n",
    "# def load_predictions(file_path):\n",
    "#     predictions = []\n",
    "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             line = line.strip()\n",
    "#             if line:\n",
    "#                 predictions.append(json.loads(line))\n",
    "#     return predictions\n",
    "\n",
    "\n",
    "# def fix_list_format(item: str) -> Any:\n",
    "#     \"\"\"\n",
    "#     Standardize string representations of lists, adding quotes around elements if missing,\n",
    "#     and safely evaluate to Python list. Returns original item if parsing fails.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(item, str):\n",
    "#         return item\n",
    "#     match = re.match(r\"^\\[(.*)\\]$\", item.strip())\n",
    "#     if not match:\n",
    "#         return item\n",
    "#     content = match.group(1)\n",
    "#     corrected = re.sub(r\"(?<!['\\w])(\\w[^,]*?)(?!['\\w])\", r\"'\\1'\", content)\n",
    "#     try:\n",
    "#         return ast.literal_eval(f\"[{corrected}]\")\n",
    "#     except (SyntaxError, ValueError):\n",
    "#         return item\n",
    "\n",
    "\n",
    "# def parse_to_list(text: str) -> Optional[List[str]]:\n",
    "#     \"\"\"\n",
    "#     Parses text to a list of strings if possible; strips quotes and whitespace.\n",
    "#     \"\"\"\n",
    "#     if not isinstance(text, str):\n",
    "#         return None\n",
    "#     try:\n",
    "#         parsed = ast.literal_eval(text)\n",
    "#     except Exception:\n",
    "#         return None\n",
    "#     if isinstance(parsed, list):\n",
    "#         return [str(x).strip(\" '\") for x in parsed]\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def to_float(text: str) -> Optional[float]:\n",
    "#     \"\"\"\n",
    "#     Converts text to float, stripping percent signs. Returns None on failure.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         return float(text.strip().strip('%'))\n",
    "#     except ValueError:\n",
    "#         return None\n",
    "\n",
    "\n",
    "# def evaluate_single_answer(\n",
    "#     target: str,\n",
    "#     prediction: str,\n",
    "#     max_relative_change: float = 0.05\n",
    "# ) -> float:\n",
    "#     \"\"\"\n",
    "#     Evaluates a single target-prediction pair:\n",
    "#     - Numeric within tolerance or exact year match inside this helper.\n",
    "#     - Falls back to ANLS for text.\n",
    "#     \"\"\"\n",
    "#     t = target.strip().strip('%').strip()\n",
    "#     p = prediction.strip().strip('%').strip()\n",
    "#     #print(\"Stripped\", t, p)\n",
    "#     # Attempt numeric\n",
    "#     t_f = to_float(t)\n",
    "#     p_f = to_float(p)\n",
    "#     if t_f is not None and p_f is not None:\n",
    "#         if t_f == 0.0:\n",
    "#             return 1.0 if p_f == 0.0 else 0.0\n",
    "#         change = abs(p_f - t_f) / abs(t_f)\n",
    "#         return 1.0 if change <= max_relative_change else 0.0\n",
    "#     # Fallback text\n",
    "#     #print(\"P:\", p, \"T: \", t)\n",
    "#     return anls_score(prediction=p.lower(), gold_labels=[t.lower()], threshold=0.5)\n",
    "\n",
    "\n",
    "# def relaxed_correctness_chartqapro(\n",
    "#     target: str,\n",
    "#     prediction: str,\n",
    "#     max_relative_change: float = 0.05,\n",
    "#     year_flags: Optional[List[bool]] = None,\n",
    "#     always_use_exact_match: bool = False,\n",
    "# ) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculates relaxed correctness between target and prediction.\n",
    "#     Supports list inputs; uses year_flags to override year handling.\n",
    "#     \"\"\"\n",
    "#     fixed_t = fix_list_format(target)\n",
    "#     t_list = parse_to_list(str(fixed_t)) or [str(target)]\n",
    "#     p_list = parse_to_list(str(prediction)) or [str(prediction)]\n",
    "#     n = len(t_list)\n",
    "#     # Expand year_flags for questions with multiple answers. \n",
    "#     if year_flags is not None and len(year_flags) < n:\n",
    "#         year_flags = year_flags * n\n",
    "\n",
    "#     # Evaluate elements\n",
    "#     scores: List[float] = []\n",
    "#     # print(t_list, p_list)\n",
    "#     for idx in range(max(len(t_list), len(p_list))):\n",
    "#         if idx >= len(t_list) or idx >= len(p_list):\n",
    "#             # Model predicted more or less elements that necessary. \n",
    "#             scores.append(0.0)\n",
    "#             continue\n",
    "#         t_item, p_item, flag = t_list[idx], p_list[idx], year_flags[idx]\n",
    "#         flag_cond = True if flag.upper()=='YES' else False\n",
    "#         if flag_cond or always_use_exact_match:\n",
    "#             # Exact integer match for years, fact checking, or multichoice\n",
    "#             try:\n",
    "#                 scores.append(1.0 if t_item.strip().lower() == p_item.strip().lower() else 0.0)\n",
    "#             except ValueError:\n",
    "#                 scores.append(0.0)\n",
    "#         else:\n",
    "#             scores.append(\n",
    "#                 evaluate_single_answer(t_item, p_item, max_relative_change)\n",
    "#             )\n",
    "#     return sum(scores) / len(scores) if scores else 0.0\n",
    "\n",
    "\n",
    "# def evaluate_predictions_chartqapro(predictions, pred_key='prediction'):\n",
    "#   gts = [x['Answer'][-1].strip(\".\").strip(\"\\n\") for x in predictions]\n",
    "#   preds = [x[pred_key].strip(\".\").strip(\"\\n\") for x in predictions]\n",
    "#   splits = [x['Question Type'] for x in predictions]\n",
    "#   year_flags =  [x['Year'] for x in predictions]\n",
    "#   # Calculate accuracy by splits\n",
    "#   match_nums_per_split = {}\n",
    "#   match_nums = []\n",
    "#   for gt, pred, split, year_flags_per_row in zip(gts, preds, splits, year_flags):\n",
    "#     # check split and calculate\n",
    "#     if split == 'Conversational':\n",
    "#       year_flags_per_row = year_flags_per_row[-1:]\n",
    "#     if split not in match_nums_per_split:\n",
    "#       match_nums_per_split[split] = []\n",
    "\n",
    "#     always_use_exact_match = True if split in ['Fact Checking', 'Multi Choice'] else False\n",
    "#     score = relaxed_correctness_chartqapro(gt, pred, year_flags=year_flags_per_row)\n",
    "#     #print(gt, pred, year_flags_per_row, score)\n",
    "#     match_nums_per_split[split].append(score)\n",
    "#     match_nums.append(score)\n",
    "\n",
    "#   final_numbers = {}\n",
    "#   for split in match_nums_per_split:\n",
    "#     final_numbers[split] = sum(match_nums_per_split[split]) / len(match_nums_per_split[split])\n",
    "#   final_numbers['Overall'] = sum(match_nums) / len(match_nums)\n",
    "#   return final_numbers\n",
    "\n",
    "\n",
    "\n",
    "#     # parser = argparse.ArgumentParser(description=\"Evaluate ChartQAPro predictions.\")\n",
    "#     # parser.add_argument(\n",
    "#     #     \"--predictions-file\",\n",
    "#     #     type=str,\n",
    "#     #     required=True,\n",
    "#     #     help=\"Path to the JSON file containing model predictions.\"\n",
    "#     # )\n",
    "#     # args = parser.parse_args()\n",
    "#     # predictions = load_predictions(args.predictions_file)\n",
    "#     # scores = evaluate_predictions_chartqapro(predictions)\n",
    "#     # print(\"ðŸ“Š Evaluation Results:\")\n",
    "#     # for k, v in scores.items():\n",
    "#     #     print(f\"  â€¢ {k:<15}: {v * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16250f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = load_predictions(\"D:\\\\AI\\\\KLTN\\\\chartqa_qwenvl_test_predictions.jsonl\")\n",
    "# scores = evaluate_predictions_chartqapro(predictions)\n",
    "# print(\"ðŸ“Š Evaluation Results:\")\n",
    "# for k, v in scores.items():\n",
    "#     print(f\"  â€¢ {k:<15}: {v * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
