{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install inflect nltk tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIra6lSkUj9e",
        "outputId": "ad1a5d68-a589-48c5-8467-bc79245d760a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting inflect\n",
            "  Downloading inflect-7.5.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.2)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/lib/python3/dist-packages (from inflect) (8.10.0)\n",
            "Collecting typeguard>=4.0.1 (from inflect)\n",
            "  Downloading typeguard-4.4.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2026.1.15)\n",
            "Requirement already satisfied: typing_extensions>=4.14.0 in /usr/local/lib/python3.12/dist-packages (from typeguard>=4.0.1->inflect) (4.15.0)\n",
            "Downloading inflect-7.5.0-py3-none-any.whl (35 kB)\n",
            "Downloading typeguard-4.4.4-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: typeguard, inflect\n",
            "Successfully installed inflect-7.5.0 typeguard-4.4.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p coco/annotations"
      ],
      "metadata": {
        "id": "ImYwu4DZc-O9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Downloading annotations...\")\n",
        "!wget -c http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
        "!unzip -q annotations_trainval2014.zip -d coco/\n",
        "!rm annotations_trainval2014.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCoAE3tCc_gX",
        "outputId": "f5b7108b-60be-409f-99be-39e743436b0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading annotations...\n",
            "--2026-02-05 20:17:40--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 3.5.29.243, 16.15.199.117, 16.15.204.228, ...\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|3.5.29.243|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: ‘annotations_trainval2014.zip’\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  55.0MB/s    in 4.8s    \n",
            "\n",
            "2026-02-05 20:17:45 (50.4 MB/s) - ‘annotations_trainval2014.zip’ saved [252872794/252872794]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Tải bộ dữ liệu cần thiết\n",
        "try:\n",
        "    nltk.download('punkt')\n",
        "    nltk.download('punkt_tab') # Đây là cái lỗi đang yêu cầu\n",
        "except Exception as e:\n",
        "    print(f\"Lỗi khi tải NLTK data: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2Cy2vD_ePFa",
        "outputId": "f490175b-0fe2-4fb3-a5cf-d1a28e829c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0EWcLtOSypu",
        "outputId": "544e09f8-d7ed-4fb2-aad7-be84444a4677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded evaluator from cache: chair.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 300/300 [00:02<00:00, 125.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Chair metrics\n",
            "CHAIRs    : 20.3\n",
            "CHAIRi    : 7.7\n",
            "Recall    : 59.4\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Copied from: https://github.com/LisaAnne/Hallucination/blob/master/utils/chair.py\n",
        "\n",
        "Modified by: Maxlinn\n",
        "\n",
        "1. adapt calculation of CHAIR-i and CHAIR-s for Python3, supports for both json and jsonl file input.\n",
        "2. integrate synonyms.txt to make the script standalone.\n",
        "3. remove machine-translation based metrics BLEU-n, CIDEr, ROGUE\n",
        "4. add new metric Recall, which represents the node words(i.e. lemmas of objects) coverage overall.\n",
        "5. add pickle cache mechanism to make it fast for repetitive evaluations.\n",
        "'''\n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import nltk\n",
        "import json\n",
        "# from pattern.en import singularize\n",
        "import argparse\n",
        "import tqdm\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "# --- BẮT ĐẦU PHẦN SỬA ĐỔI ---\n",
        "import inflect\n",
        "_p_inflect = inflect.engine()\n",
        "\n",
        "def singularize(word):\n",
        "    \"\"\"\n",
        "    Hàm thay thế cho pattern.en.singularize sử dụng thư viện inflect.\n",
        "    p.singular_noun trả về từ số ít nếu input là số nhiều, hoặc False nếu input đã là số ít.\n",
        "    \"\"\"\n",
        "    if word is None: return \"\"\n",
        "    return _p_inflect.singular_noun(word) or word\n",
        "\n",
        "# copied from: https://github.com/LisaAnne/Hallucination/blob/master/data/synonyms.txt\n",
        "synonyms_txt = '''\n",
        "person, girl, boy, man, woman, kid, child, chef, baker, people, adult, rider, children, baby, worker, passenger, sister, biker, policeman, cop, officer, lady, cowboy, bride, groom, male, female, guy, traveler, mother, father, gentleman, pitcher, player, skier, snowboarder, skater, skateboarder, person, woman, guy, foreigner, child, gentleman, caller, offender, coworker, trespasser, patient, politician, soldier, grandchild, serviceman, walker, drinker, doctor, bicyclist, thief, buyer, teenager, student, camper, driver, solider, hunter, shopper, villager\n",
        "bicycle, bike, bicycle, bike, unicycle, minibike, trike\n",
        "car, automobile, van, minivan, sedan, suv, hatchback, cab, jeep, coupe, taxicab, limo, taxi\n",
        "motorcycle, scooter,  motor bike, motor cycle, motorbike, scooter, moped\n",
        "airplane, jetliner, plane, air plane, monoplane, aircraft, jet, jetliner, airbus, biplane, seaplane\n",
        "bus, minibus, trolley\n",
        "train, locomotive, tramway, caboose\n",
        "truck, pickup, lorry, hauler, firetruck\n",
        "boat, ship, liner, sailboat, motorboat, dinghy, powerboat, speedboat, canoe, skiff, yacht, kayak, catamaran, pontoon, houseboat, vessel, rowboat, trawler, ferryboat, watercraft, tugboat, schooner, barge, ferry, sailboard, paddleboat, lifeboat, freighter, steamboat, riverboat, battleship, steamship\n",
        "traffic light, street light, traffic signal, stop light, streetlight, stoplight\n",
        "fire hydrant, hydrant\n",
        "stop sign\n",
        "parking meter\n",
        "bench, pew\n",
        "bird, ostrich, owl, seagull, goose, duck, parakeet, falcon, robin, pelican, waterfowl, heron, hummingbird, mallard, finch, pigeon, sparrow, seabird, osprey, blackbird, fowl, shorebird, woodpecker, egret, chickadee, quail, bluebird, kingfisher, buzzard, willet, gull, swan, bluejay, flamingo, cormorant, parrot, loon, gosling, waterbird, pheasant, rooster, sandpiper, crow, raven, turkey, oriole, cowbird, warbler, magpie, peacock, cockatiel, lorikeet, puffin, vulture, condor, macaw, peafowl, cockatoo, songbird\n",
        "cat, kitten, feline, tabby\n",
        "dog, puppy, beagle, pup, chihuahua, schnauzer, dachshund, rottweiler, canine, pitbull, collie, pug, terrier, poodle, labrador, doggie, doberman, mutt, doggy, spaniel, bulldog, sheepdog, weimaraner, corgi, cocker, greyhound, retriever, brindle, hound, whippet, husky\n",
        "horse, colt, pony, racehorse, stallion, equine, mare, foal, palomino, mustang, clydesdale, bronc, bronco\n",
        "sheep, lamb, ram, lamb, goat, ewe\n",
        "cow, cattle, oxen, ox, calf, cattle, holstein, heifer, buffalo, bull, zebu, bison\n",
        "elephant\n",
        "bear, panda\n",
        "zebra\n",
        "giraffe\n",
        "backpack, knapsack\n",
        "umbrella\n",
        "handbag, wallet, purse, briefcase\n",
        "tie, bow, bow tie\n",
        "suitcase, suit case, luggage\n",
        "frisbee\n",
        "skis, ski\n",
        "snowboard\n",
        "sports ball, ball\n",
        "kite\n",
        "baseball bat\n",
        "baseball glove\n",
        "skateboard\n",
        "surfboard, longboard, skimboard, shortboard, wakeboard\n",
        "tennis racket, racket\n",
        "bottle\n",
        "wine glass\n",
        "cup\n",
        "fork\n",
        "knife, pocketknife, knive\n",
        "spoon\n",
        "bowl, container\n",
        "banana\n",
        "apple\n",
        "sandwich, burger, sub, cheeseburger, hamburger\n",
        "orange\n",
        "broccoli\n",
        "carrot\n",
        "hot dog\n",
        "pizza\n",
        "donut, doughnut, bagel\n",
        "cake,  cheesecake, cupcake, shortcake, coffeecake, pancake\n",
        "chair, seat, stool\n",
        "couch, sofa, recliner, futon, loveseat, settee, chesterfield\n",
        "potted plant, houseplant\n",
        "bed\n",
        "dining table, table, desk\n",
        "toilet, urinal, commode, toilet, lavatory, potty\n",
        "tv, monitor, televison, television\n",
        "laptop, computer, notebook, netbook, lenovo, macbook, laptop computer\n",
        "mouse\n",
        "remote\n",
        "keyboard\n",
        "cell phone, mobile phone, phone, cellphone, telephone, phon, smartphone, iPhone\n",
        "microwave\n",
        "oven, stovetop, stove, stove top oven\n",
        "toaster\n",
        "sink\n",
        "refrigerator, fridge, fridge, freezer\n",
        "book\n",
        "clock\n",
        "vase\n",
        "scissors\n",
        "teddy bear, teddybear\n",
        "hair drier, hairdryer\n",
        "toothbrush\n",
        "'''\n",
        "\n",
        "\n",
        "def combine_coco_captions(annotation_path):\n",
        "\n",
        "    if not os.path.exists('%s/captions_%s2014.json' %(annotation_path, 'val')):\n",
        "        raise Exception(\"Please download MSCOCO caption annotations for val set\")\n",
        "    if not os.path.exists('%s/captions_%s2014.json' %(annotation_path, 'train')):\n",
        "        raise Exception(\"Please download MSCOCO caption annotations for train set\")\n",
        "\n",
        "    val_caps = json.load(open('%s/captions_%s2014.json' %(annotation_path, 'val')))\n",
        "    train_caps = json.load(open('%s/captions_%s2014.json' %(annotation_path, 'train')))\n",
        "    all_caps = {'info': train_caps['info'],\n",
        "                'licenses': train_caps['licenses'],\n",
        "                'images': val_caps['images'] + train_caps['images'],\n",
        "                'annotations': val_caps['annotations'] + train_caps['annotations']}\n",
        "\n",
        "    return all_caps\n",
        "\n",
        "def combine_coco_instances(annotation_path):\n",
        "\n",
        "    if not os.path.exists('%s/instances_%s2014.json' %(annotation_path, 'val')):\n",
        "        raise Exception(\"Please download MSCOCO instance annotations for val set\")\n",
        "    if not os.path.exists('%s/instances_%s2014.json' %(annotation_path, 'train')):\n",
        "        raise Exception(\"Please download MSCOCO instance annotations for train set\")\n",
        "\n",
        "    val_instances = json.load(open('%s/instances_%s2014.json' %(annotation_path, 'val')))\n",
        "    train_instances = json.load(open('%s/instances_%s2014.json' %(annotation_path, 'train')))\n",
        "    all_instances = {'info': train_instances['info'],\n",
        "                     'licenses': train_instances['licenses'],\n",
        "                     'type': train_instances['licenses'],\n",
        "                     'categories': train_instances['categories'],\n",
        "                     'images': train_instances['images'] + val_instances['images'],\n",
        "                     'annotations': val_instances['annotations'] + train_instances['annotations']}\n",
        "\n",
        "    return all_instances\n",
        "\n",
        "class CHAIR(object):\n",
        "\n",
        "    def __init__(self, coco_path):\n",
        "\n",
        "        self.imid_to_objects = defaultdict(list) # later become a dict of sets\n",
        "\n",
        "        self.coco_path = coco_path\n",
        "\n",
        "        #read in synonyms\n",
        "        synonyms = synonyms_txt.splitlines()\n",
        "        synonyms = [s.strip().split(', ') for s in synonyms]\n",
        "        self.mscoco_objects = [] #mscoco objects and *all* synonyms\n",
        "        self.inverse_synonym_dict = {}\n",
        "        for synonym in synonyms:\n",
        "            self.mscoco_objects.extend(synonym)\n",
        "            for s in synonym:\n",
        "                self.inverse_synonym_dict[s] = synonym[0]\n",
        "\n",
        "        #Some hard coded rules for implementing CHAIR metrics on MSCOCO\n",
        "\n",
        "        #common 'double words' in MSCOCO that should be treated as a single word\n",
        "        coco_double_words = ['motor bike', 'motor cycle', 'air plane', 'traffic light', 'street light', 'traffic signal', 'stop light', 'fire hydrant', 'stop sign', 'parking meter', 'suit case', 'sports ball', 'baseball bat', 'baseball glove', 'tennis racket', 'wine glass', 'hot dog', 'cell phone', 'mobile phone', 'teddy bear', 'hair drier', 'potted plant', 'bow tie', 'laptop computer', 'stove top oven', 'hot dog', 'teddy bear', 'home plate', 'train track']\n",
        "\n",
        "        #Hard code some rules for special cases in MSCOCO\n",
        "        #qualifiers like 'baby' or 'adult' animal will lead to a false fire for the MSCOCO object 'person'.  'baby bird' --> 'bird'.\n",
        "        animal_words = ['bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'animal', 'cub']\n",
        "        #qualifiers like 'passenger' vehicle will lead to a false fire for the MSCOCO object 'person'.  'passenger jet' --> 'jet'.\n",
        "        vehicle_words = ['jet', 'train']\n",
        "\n",
        "        #double_word_dict will map double words to the word they should be treated as in our analysis\n",
        "\n",
        "        self.double_word_dict = {}\n",
        "        for double_word in coco_double_words:\n",
        "            self.double_word_dict[double_word] = double_word\n",
        "        for animal_word in animal_words:\n",
        "            self.double_word_dict['baby %s' %animal_word] = animal_word\n",
        "            self.double_word_dict['adult %s' %animal_word] = animal_word\n",
        "        for vehicle_word in vehicle_words:\n",
        "            self.double_word_dict['passenger %s' %vehicle_word] = vehicle_word\n",
        "        self.double_word_dict['bow tie'] = 'tie'\n",
        "        self.double_word_dict['toilet seat'] = 'toilet'\n",
        "        self.double_word_dict['wine glas'] = 'wine glass'\n",
        "\n",
        "        self.get_annotations()\n",
        "\n",
        "    def _load_generated_captions_into_evaluator(self, cap_file, image_id_key, caption_key):\n",
        "\n",
        "        '''\n",
        "        Meant to save time so imid_to_objects does not always need to be recomputed.\n",
        "        '''\n",
        "        #Read in captions\n",
        "        self.caps, self.eval_imids = load_generated_captions(cap_file, image_id_key, caption_key)\n",
        "        assert len(self.caps) == len(self.eval_imids)\n",
        "\n",
        "\n",
        "    def caption_to_words(self, caption):\n",
        "\n",
        "        '''\n",
        "        Input: caption\n",
        "        Output: MSCOCO words in the caption\n",
        "        '''\n",
        "\n",
        "        #standard preprocessing\n",
        "        words = nltk.word_tokenize(caption.lower())\n",
        "        words = [singularize(w) for w in words]\n",
        "\n",
        "        #replace double words\n",
        "        i = 0\n",
        "        double_words = []\n",
        "        idxs = []\n",
        "        while i < len(words):\n",
        "           idxs.append(i)\n",
        "           double_word = ' '.join(words[i:i+2])\n",
        "           if double_word in self.double_word_dict:\n",
        "               double_words.append(self.double_word_dict[double_word])\n",
        "               i += 2\n",
        "           else:\n",
        "               double_words.append(words[i])\n",
        "               i += 1\n",
        "        words = double_words\n",
        "\n",
        "        #toilet seat is not chair (sentences like \"the seat of the toilet\" will fire for \"chair\" if we do not include this line)\n",
        "        if ('toilet' in words) & ('seat' in words): words = [word for word in words if word != 'seat']\n",
        "\n",
        "        #get synonyms for all words in the caption\n",
        "        idxs = [idxs[idx] for idx, word in enumerate(words) \\\n",
        "                if word in set(self.mscoco_objects)]\n",
        "        words = [word for word in words if word in set(self.mscoco_objects)]\n",
        "        node_words = []\n",
        "        for word in words:\n",
        "            node_words.append(self.inverse_synonym_dict[word])\n",
        "        #return all the MSCOCO objects in the caption\n",
        "        return words, node_words, idxs, double_words\n",
        "\n",
        "    def get_annotations_from_segments(self):\n",
        "        '''\n",
        "        Add objects taken from MSCOCO segmentation masks\n",
        "        '''\n",
        "\n",
        "        coco_segments = combine_coco_instances(self.coco_path )\n",
        "        segment_annotations = coco_segments['annotations']\n",
        "\n",
        "        #make dict linking object name to ids\n",
        "        id_to_name = {} #dict with id to synsets\n",
        "        for cat in coco_segments['categories']:\n",
        "            id_to_name[cat['id']] = cat['name']\n",
        "\n",
        "        for i, annotation in enumerate(segment_annotations):\n",
        "            sys.stdout.write(\"\\rGetting annotations for %d/%d segmentation masks\"\n",
        "                              %(i, len(segment_annotations)))\n",
        "            imid = annotation['image_id']\n",
        "\n",
        "            node_word = self.inverse_synonym_dict[id_to_name[annotation['category_id']]]\n",
        "            self.imid_to_objects[imid].append(node_word)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    def get_annotations_from_captions(self):\n",
        "        '''\n",
        "        Add objects taken from MSCOCO ground truth captions\n",
        "        '''\n",
        "\n",
        "        coco_caps = combine_coco_captions(self.coco_path)\n",
        "        caption_annotations = coco_caps['annotations']\n",
        "\n",
        "        for i, annotation in enumerate(caption_annotations):\n",
        "            sys.stdout.write('\\rGetting annotations for %d/%d ground truth captions'\n",
        "                              %(i, len(coco_caps['annotations'])))\n",
        "            imid = annotation['image_id']\n",
        "\n",
        "            _, node_words, _, _ = self.caption_to_words(annotation['caption'])\n",
        "            # note here is update, so call get_annotations_from_segments first\n",
        "            self.imid_to_objects[imid].extend(node_words)\n",
        "        print(\"\\n\")\n",
        "\n",
        "\n",
        "    def get_annotations(self):\n",
        "\n",
        "        '''\n",
        "        Get annotations from both segmentation and captions.  Need both annotation types for CHAIR metric.\n",
        "        '''\n",
        "\n",
        "        self.get_annotations_from_segments()\n",
        "        self.get_annotations_from_captions()\n",
        "        # deduplicate\n",
        "        for imid in self.imid_to_objects:\n",
        "            self.imid_to_objects[imid] = set(self.imid_to_objects[imid])\n",
        "\n",
        "    def compute_chair(self, cap_file, image_id_key, caption_key):\n",
        "        '''\n",
        "        Given ground truth objects and generated captions, determine which sentences have hallucinated words.\n",
        "        '''\n",
        "        self._load_generated_captions_into_evaluator(cap_file, image_id_key, caption_key)\n",
        "\n",
        "        imid_to_objects = self.imid_to_objects\n",
        "        caps = self.caps\n",
        "        eval_imids = self.eval_imids\n",
        "\n",
        "        num_caps = 0.\n",
        "        num_hallucinated_caps = 0.\n",
        "        hallucinated_word_count = 0.\n",
        "        coco_word_count = 0.\n",
        "\n",
        "        # :add:\n",
        "        num_recall_gt_objects = 0.\n",
        "        num_gt_objects = 0.\n",
        "\n",
        "        output = {'sentences': []}\n",
        "\n",
        "        for i in tqdm.trange(len(caps)):\n",
        "            cap :str = caps[i]\n",
        "            imid :int = eval_imids[i]\n",
        "\n",
        "            #get all words in the caption, as well as corresponding node word\n",
        "            words, node_words, idxs, raw_words = self.caption_to_words(cap)\n",
        "\n",
        "            gt_objects = imid_to_objects[imid]\n",
        "            cap_dict = {'image_id': imid,\n",
        "                        'caption': cap,\n",
        "                        'mscoco_hallucinated_words': [],\n",
        "                        'mscoco_gt_words': list(gt_objects),\n",
        "                        'mscoco_generated_words': list(node_words),\n",
        "                        'hallucination_idxs': [],\n",
        "                        'words': raw_words\n",
        "                        }\n",
        "\n",
        "            # :add:\n",
        "            cap_dict['metrics'] = {'CHAIRs': 0,\n",
        "                                   'CHAIRi': 0,\n",
        "                                   'Recall': 0}\n",
        "\n",
        "            #count hallucinated words\n",
        "            coco_word_count += len(node_words)\n",
        "            hallucinated = False\n",
        "\n",
        "            # add\n",
        "            recall_gt_objects = set()\n",
        "            for word, node_word, idx in zip(words, node_words, idxs):\n",
        "                if node_word not in gt_objects:\n",
        "                    hallucinated_word_count += 1\n",
        "                    cap_dict['mscoco_hallucinated_words'].append((word, node_word))\n",
        "                    cap_dict['hallucination_idxs'].append(idx)\n",
        "                    hallucinated = True\n",
        "                else:\n",
        "                    recall_gt_objects.add(node_word)\n",
        "\n",
        "            #count hallucinated caps\n",
        "            num_caps += 1\n",
        "            if hallucinated:\n",
        "               num_hallucinated_caps += 1\n",
        "\n",
        "            # add\n",
        "            num_gt_objects += len(gt_objects)\n",
        "            num_recall_gt_objects += len(recall_gt_objects)\n",
        "\n",
        "            cap_dict['metrics']['CHAIRs'] = int(hallucinated)\n",
        "            cap_dict['metrics']['CHAIRi'] = 0.\n",
        "            cap_dict['metrics']['Recall'] = 0.\n",
        "\n",
        "            if len(words) > 0:\n",
        "                cap_dict['metrics']['CHAIRi'] = len(cap_dict['mscoco_hallucinated_words'])/float(len(words))\n",
        "\n",
        "            # add\n",
        "            if len(gt_objects) > 0:\n",
        "                cap_dict['metrics']['Recall'] = len(recall_gt_objects) / len(gt_objects)\n",
        "\n",
        "            output['sentences'].append(cap_dict)\n",
        "\n",
        "        chair_s = (num_hallucinated_caps/num_caps)\n",
        "        chair_i = (hallucinated_word_count/coco_word_count)\n",
        "        # add\n",
        "        recall = num_recall_gt_objects / num_gt_objects\n",
        "\n",
        "        output['overall_metrics'] = {'CHAIRs': chair_s,\n",
        "                                     'CHAIRi': chair_i,\n",
        "                                     'Recall': recall}\n",
        "\n",
        "        return output\n",
        "\n",
        "def load_generated_captions(cap_file, image_id_key:str, caption_key:str):\n",
        "    #Read in captions\n",
        "    # it should be list of dict\n",
        "    ext = os.path.splitext(cap_file)[-1]\n",
        "    if ext == '.json':\n",
        "        caps = json.load(open(cap_file))\n",
        "    elif ext == '.jsonl':\n",
        "        caps = [json.loads(s) for s in open(cap_file)]\n",
        "    else:\n",
        "        raise ValueError(f'Unspported extension {ext} for cap_file: {cap_file}')\n",
        "\n",
        "    # list of int\n",
        "    imids = [obj[image_id_key] for obj in caps]\n",
        "\n",
        "    # list of str\n",
        "    caps = [obj[caption_key] for obj in caps]\n",
        "\n",
        "    return caps, imids\n",
        "\n",
        "def save_hallucinated_words(cap_file, cap_dict):\n",
        "    with open(cap_file, 'w') as f:\n",
        "        json.dump(cap_dict, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def print_metrics(hallucination_cap_dict, quiet=False):\n",
        "    sentence_metrics = hallucination_cap_dict['overall_metrics']\n",
        "\n",
        "    for k, v in sentence_metrics.items():\n",
        "        k_str = str(k).ljust(10)\n",
        "        v_str = f'{v * 100:.01f}'\n",
        "        print(k_str, v_str, sep=': ')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--cap_file\", type=str, default='',\n",
        "                        help=\"path towards json or jsonl saving image ids and their captions in list of dict.\")\n",
        "    parser.add_argument(\"--image_id_key\", type=str, default=\"image_id\",\n",
        "                        help=\"in each dict of cap_file, which key stores image id of coco.\")\n",
        "    parser.add_argument(\"--caption_key\", type=str, default=\"caption\",\n",
        "                        help=\"in each dict of cap_file, which key stores caption of the image.\")\n",
        "\n",
        "    parser.add_argument(\"--cache\", type=str, default=\"chair.pkl\",\n",
        "                        help=\"pre inited CHAIR evaluator object, for fast loading.\")\n",
        "    parser.add_argument(\"--coco_path\", type=str, default='coco_annotations',\n",
        "                        help=\"only use for regenerating CHAIR evaluator object, will be ignored if uses cached evaluator.\")\n",
        "\n",
        "    parser.add_argument(\"--save_path\", type=str, default=\"\",\n",
        "                        help=\"saving CHAIR evaluate and results to json, useful for debugging the caption model.\")\n",
        "\n",
        "    # Modify parser.parse_args() to handle Colab's internal arguments\n",
        "    args = parser.parse_args(args=[\n",
        "        '--cap_file', '/content/m3id_results_coco_val.json',\n",
        "        '--coco_path', '/content/coco/annotations/',\n",
        "        '--save_path', 'output.json'\n",
        "    ])\n",
        "\n",
        "    # Example of how to set arguments programmatically if running in Colab\n",
        "    # args.cap_file = 'your_caption_file.json'\n",
        "    # args.image_id_key = 'your_image_id_key'\n",
        "    # args.caption_key = 'your_caption_key'\n",
        "    # args.coco_path = 'your_coco_annotations_path'\n",
        "    # args.save_path = 'your_results.json'\n",
        "\n",
        "    if args.cache and os.path.exists(args.cache):\n",
        "        evaluator = pickle.load(open(args.cache, 'rb'))\n",
        "        print(f\"loaded evaluator from cache: {args.cache}\")\n",
        "    else:\n",
        "        print(f\"cache not setted or not exist yet, building from scratch...\")\n",
        "        evaluator = CHAIR(args.coco_path)\n",
        "        pickle.dump(evaluator, open(args.cache, 'wb'))\n",
        "        print(f\"cached evaluator to: {args.cache}\")\n",
        "\n",
        "    cap_dict = evaluator.compute_chair(args.cap_file, args.image_id_key, args.caption_key)\n",
        "\n",
        "    print(\"\\nChair metrics\")\n",
        "    print_metrics(cap_dict)\n",
        "\n",
        "    if args.save_path:\n",
        "        save_hallucinated_words(args.save_path, cap_dict)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "unRA7yw4brc7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}