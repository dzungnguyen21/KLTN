{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip3 install bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:13:58.267780Z",
     "iopub.status.busy": "2025-01-05T15:13:58.267563Z",
     "iopub.status.idle": "2025-01-05T15:14:08.471624Z",
     "shell.execute_reply": "2025-01-05T15:14:08.470642Z",
     "shell.execute_reply.started": "2025-01-05T15:13:58.267759Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from transformers import Qwen2VLForConditionalGeneration, Qwen2VLProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:14:08.475742Z",
     "iopub.status.busy": "2025-01-05T15:14:08.475462Z",
     "iopub.status.idle": "2025-01-05T15:14:08.506712Z",
     "shell.execute_reply": "2025-01-05T15:14:08.505786Z",
     "shell.execute_reply.started": "2025-01-05T15:14:08.475713Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "MODEL_ID = \"Qwen/Qwen2-VL-7B-Instruct\"\n",
    "EPOCHS = 5\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_CHECKPOINTING = True,  # Tradeoff between memory efficiency and computation time.\n",
    "USE_REENTRANT = False,\n",
    "OPTIM = \"paged_adamw_32bit\"\n",
    "LEARNING_RATE = 2e-5\n",
    "LOGGING_STEPS = 50\n",
    "EVAL_STEPS = 50\n",
    "SAVE_STEPS = 50\n",
    "EVAL_STRATEGY = \"steps\"\n",
    "SAVE_STRATEGY = \"steps\"\n",
    "METRIC_FOR_BEST_MODEL=\"eval_loss\"\n",
    "LOAD_BEST_MODEL_AT_END=True\n",
    "MAX_GRAD_NORM = 1\n",
    "WARMUP_STEPS = 0\n",
    "DATASET_KWARGS={\"skip_prepare_dataset\": True} # We have to put for VLMs\n",
    "REMOVE_UNUSED_COLUMNS = False # VLM thing\n",
    "MAX_SEQ_LEN=128\n",
    "NUM_STEPS = (283 // BATCH_SIZE) * EPOCHS\n",
    "print(f\"NUM_STEPS: {NUM_STEPS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:14:14.636519Z",
     "iopub.status.busy": "2025-01-05T15:14:14.636290Z",
     "iopub.status.idle": "2025-01-05T15:14:14.641638Z",
     "shell.execute_reply": "2025-01-05T15:14:14.640437Z",
     "shell.execute_reply.started": "2025-01-05T15:14:14.636500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "system_message = \"\"\"You are a highly advanced Vision Language Model (VLM), specialized in analyzing, describing, and interpreting visual data. \n",
    "Your task is to process and extract meaningful insights from images, videos, and visual patterns, \n",
    "leveraging multimodal understanding to provide accurate and contextually relevant information.\"\"\"\n",
    "\n",
    "def format_data(sample):\n",
    "    return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"image\": sample[\"image\"],\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": sample[\"query\"],\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": sample[\"label\"][0]}],\n",
    "        },\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:14:14.642727Z",
     "iopub.status.busy": "2025-01-05T15:14:14.642421Z",
     "iopub.status.idle": "2025-01-05T15:14:18.252800Z",
     "shell.execute_reply": "2025-01-05T15:14:18.251977Z",
     "shell.execute_reply.started": "2025-01-05T15:14:14.642696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dataset, eval_dataset, test_dataset = load_dataset(\"HuggingFaceM4/ChartQA\", \n",
    "                                                         split=[\"train[:1%]\", \"val[:1%]\", \"test[:1%]\"])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(\"-\"*30)\n",
    "print(train_dataset)\n",
    "print(\"-\"*30)\n",
    "print(train_dataset[0])\n",
    "print(\"-\"*30)\n",
    "\n",
    "train_dataset = [format_data(sample) for sample in train_dataset]\n",
    "eval_dataset = [format_data(sample) for sample in eval_dataset]\n",
    "test_dataset = [format_data(sample) for sample in test_dataset]\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(\"-\"*30)\n",
    "print(train_dataset[0])\n",
    "print(\"-\"*30)\n",
    "print(len(test_dataset))\n",
    "print(\"-\"*30)\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:14:18.253942Z",
     "iopub.status.busy": "2025-01-05T15:14:18.253624Z",
     "iopub.status.idle": "2025-01-05T15:14:18.297145Z",
     "shell.execute_reply": "2025-01-05T15:14:18.296258Z",
     "shell.execute_reply.started": "2025-01-05T15:14:18.253909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sample_data = test_dataset[0]\n",
    "sample_question = test_dataset[0][1][\"content\"][1][\"text\"]\n",
    "sample_answer = test_dataset[0][2][\"content\"][0][\"text\"]\n",
    "sample_image = test_dataset[0][1][\"content\"][0][\"image\"]\n",
    "\n",
    "print(sample_question)\n",
    "print(sample_answer)\n",
    "sample_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:14:18.300278Z",
     "iopub.status.busy": "2025-01-05T15:14:18.300024Z",
     "iopub.status.idle": "2025-01-05T15:15:39.039842Z",
     "shell.execute_reply": "2025-01-05T15:15:39.039104Z",
     "shell.execute_reply.started": "2025-01-05T15:14:18.300252Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=False\n",
    "        )\n",
    "\n",
    "else:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        use_cache=False\n",
    "        )\n",
    "    \n",
    "processor = Qwen2VLProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:18:30.318545Z",
     "iopub.status.busy": "2025-01-05T15:18:30.318143Z",
     "iopub.status.idle": "2025-01-05T15:23:19.375846Z",
     "shell.execute_reply": "2025-01-05T15:23:19.374796Z",
     "shell.execute_reply.started": "2025-01-05T15:18:30.318515Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def text_generator(sample_data):\n",
    "    text = processor.apply_chat_template(\n",
    "        sample_data[0:2], tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    print(f\"Prompt: {text}\")\n",
    "    print(\"-\"*30)\n",
    "\n",
    "    image_inputs = sample_data[1][\"content\"][0][\"image\"]\n",
    "\n",
    "    inputs = processor(\n",
    "        text=[text],\n",
    "        images = image_inputs,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    generated_ids = model.generate(**inputs, max_new_tokens=MAX_SEQ_LEN)\n",
    "\n",
    "    output_text = processor.batch_decode(\n",
    "        generated_ids, skip_special_tokens=True\n",
    "    )\n",
    "    del inputs\n",
    "    actual_answer = sample_data[2][\"content\"][0][\"text\"]\n",
    "    return output_text[0], actual_answer\n",
    "    \n",
    "\n",
    "generated_text, actual_answer = text_generator(sample_data)\n",
    "print(f\"Generated Answer: {generated_text}\")\n",
    "print(f\"Actual Answer: {actual_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:27:41.050270Z",
     "iopub.status.busy": "2025-01-05T15:27:41.049767Z",
     "iopub.status.idle": "2025-01-05T15:27:41.166708Z",
     "shell.execute_reply": "2025-01-05T15:27:41.165672Z",
     "shell.execute_reply.started": "2025-01-05T15:27:41.050228Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# peft_config = LoraConfig(\n",
    "#     lora_alpha=16,\n",
    "#     lora_dropout=0.1,\n",
    "#     r=8,\n",
    "#     bias=\"none\",\n",
    "#     target_modules=[\"q_proj\", \"v_proj\"],\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "# peft_model.print_trainable_parameters() # After LoRA trainable parameters increases. Since we add adapter.\n",
    "\n",
    "# Cell code thay thế cho phần cấu hình LoRA và Training Args cũ\n",
    "\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig\n",
    "\n",
    "# 1. Cấu hình LoRA/DoRA nâng cao\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05, # Giảm dropout một chút để ổn định hơn\n",
    "    r=16,              # Tăng rank lên 16 để model học được nhiều chi tiết hơn từ biểu đồ\n",
    "    bias=\"none\",\n",
    "    # KỸ THUẬT 1: Target tất cả các linear layers giúp model hiểu ảnh và suy luận tốt hơn\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # KỸ THUẬT 2: Bật DoRA (Weight-Decomposed LoRA)\n",
    "    # Lưu ý: DoRA tốn VRAM hơn một chút so với LoRA thường, nếu OOM thì set về False\n",
    "    use_dora=True \n",
    ")\n",
    "\n",
    "# 2. Cấu hình Training với NEFTune\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./output_robust\",\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "    learning_rate=2e-5, # Qwen2-VL khá nhạy cảm, giữ LR thấp\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    eval_strategy=EVAL_STRATEGY,\n",
    "    save_strategy=SAVE_STRATEGY,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "    load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "    max_grad_norm=1,\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    dataset_kwargs=DATASET_KWARGS,\n",
    "    max_length=MAX_SEQ_LEN,\n",
    "    remove_unused_columns=REMOVE_UNUSED_COLUMNS,\n",
    "    optim=OPTIM,\n",
    "    \n",
    "    # KỸ THUẬT 3: NEFTune - Thêm nhiễu vào embeddings\n",
    "    # Giá trị thường dùng là 5. Nếu dataset quá nhỏ, có thể thử 10.\n",
    "    neftune_noise_alpha=5 \n",
    ")\n",
    "\n",
    "print(\"Đã cập nhật cấu hình: Target Modules mở rộng, DoRA enabled, và NEFTune enabled.\")\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:31:09.251220Z",
     "iopub.status.busy": "2025-01-05T15:31:09.250765Z",
     "iopub.status.idle": "2025-01-05T15:31:09.288881Z",
     "shell.execute_reply": "2025-01-05T15:31:09.288228Z",
     "shell.execute_reply.started": "2025-01-05T15:31:09.251183Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# training_args = SFTConfig(\n",
    "#     output_dir=\"./output\",\n",
    "#     num_train_epochs=EPOCHS,\n",
    "#     per_device_train_batch_size=BATCH_SIZE,\n",
    "#     per_device_eval_batch_size=BATCH_SIZE,\n",
    "#     gradient_checkpointing=GRADIENT_CHECKPOINTING,\n",
    "#     learning_rate=LEARNING_RATE,\n",
    "#     logging_steps=LOGGING_STEPS,\n",
    "#     eval_steps=EVAL_STEPS,\n",
    "#     eval_strategy=EVAL_STRATEGY,\n",
    "#     save_strategy=SAVE_STRATEGY,\n",
    "#     save_steps=SAVE_STEPS,\n",
    "#     metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "#     load_best_model_at_end=LOAD_BEST_MODEL_AT_END,\n",
    "#     max_grad_norm=MAX_GRAD_NORM,\n",
    "#     warmup_steps=WARMUP_STEPS,\n",
    "#     dataset_kwargs=DATASET_KWARGS,\n",
    "#     max_length=MAX_SEQ_LEN,\n",
    "#     remove_unused_columns = REMOVE_UNUSED_COLUMNS,\n",
    "#     optim=OPTIM,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:37:31.494476Z",
     "iopub.status.busy": "2025-01-05T15:37:31.494077Z",
     "iopub.status.idle": "2025-01-05T15:37:31.531893Z",
     "shell.execute_reply": "2025-01-05T15:37:31.531034Z",
     "shell.execute_reply.started": "2025-01-05T15:37:31.494448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "collate_sample = [train_dataset[0], train_dataset[1]] # for batch size 2.\n",
    "\n",
    "def collate_fn(examples):\n",
    "    texts = [processor.apply_chat_template(example, tokenize=False) for example in examples]\n",
    "    image_inputs = [example[1][\"content\"][0][\"image\"] for example in examples]\n",
    "\n",
    "    batch = processor(\n",
    "        text=texts, images=image_inputs, return_tensors=\"pt\", padding=True\n",
    "    )\n",
    "    labels = batch[\"input_ids\"].clone()\n",
    "    labels[labels == processor.tokenizer.pad_token_id] = -100\n",
    "    batch[\"labels\"] = batch[\"input_ids\"]\n",
    "\n",
    "    return batch\n",
    "\n",
    "collated_data = collate_fn(collate_sample)\n",
    "print(collated_data.keys())  # dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:39:21.333547Z",
     "iopub.status.busy": "2025-01-05T15:39:21.333110Z",
     "iopub.status.idle": "2025-01-05T15:39:21.574428Z",
     "shell.execute_reply": "2025-01-05T15:39:21.573742Z",
     "shell.execute_reply.started": "2025-01-05T15:39:21.333516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=collate_fn,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor.tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T15:40:31.446595Z",
     "iopub.status.busy": "2025-01-05T15:40:31.446248Z",
     "iopub.status.idle": "2025-01-05T16:01:58.757399Z",
     "shell.execute_reply": "2025-01-05T16:01:58.756616Z",
     "shell.execute_reply.started": "2025-01-05T15:40:31.446571Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(\"-\"*30)\n",
    "print(\"Initial Evaluation\")\n",
    "metric = trainer.evaluate()\n",
    "print(metric)\n",
    "print(\"-\"*30)\n",
    "\n",
    "print(\"Training\")\n",
    "trainer.train()\n",
    "print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T16:06:30.581957Z",
     "iopub.status.busy": "2025-01-05T16:06:30.581599Z",
     "iopub.status.idle": "2025-01-05T16:06:31.361186Z",
     "shell.execute_reply": "2025-01-05T16:06:31.360230Z",
     "shell.execute_reply.started": "2025-01-05T16:06:30.581929Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.save_model(training_args.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T16:07:19.619619Z",
     "iopub.status.busy": "2025-01-05T16:07:19.619123Z",
     "iopub.status.idle": "2025-01-05T16:07:28.800939Z",
     "shell.execute_reply": "2025-01-05T16:07:28.800194Z",
     "shell.execute_reply.started": "2025-01-05T16:07:19.619582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "\n",
    "# https://huggingface.co/learn/cookbook/en/fine_tuning_vlm_trl\n",
    "def clear_memory():\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T16:08:25.198056Z",
     "iopub.status.busy": "2025-01-05T16:08:25.197713Z",
     "iopub.status.idle": "2025-01-05T16:09:52.910598Z",
     "shell.execute_reply": "2025-01-05T16:09:52.909605Z",
     "shell.execute_reply.started": "2025-01-05T16:08:25.198033Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if device == \"cuda\":\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        device_map=\"auto\", \n",
    "        quantization_config=bnb_config,\n",
    "        use_cache=True\n",
    "        )\n",
    "\n",
    "else:\n",
    "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
    "        MODEL_ID, \n",
    "        use_cache=True\n",
    "        )\n",
    "    \n",
    "processor = Qwen2VLProcessor.from_pretrained(MODEL_ID)\n",
    "processor.tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T16:11:32.046009Z",
     "iopub.status.busy": "2025-01-05T16:11:32.045621Z",
     "iopub.status.idle": "2025-01-05T16:11:32.251842Z",
     "shell.execute_reply": "2025-01-05T16:11:32.251065Z",
     "shell.execute_reply.started": "2025-01-05T16:11:32.045977Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Before adapter parameters: {model.num_parameters()}\")\n",
    "model.load_adapter(\"./output\")\n",
    "print(f\"After adapter parameters: {model.num_parameters()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-05T16:12:21.886086Z",
     "iopub.status.busy": "2025-01-05T16:12:21.885750Z",
     "iopub.status.idle": "2025-01-05T16:12:26.323915Z",
     "shell.execute_reply": "2025-01-05T16:12:26.322975Z",
     "shell.execute_reply.started": "2025-01-05T16:12:21.886057Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "generated_text, actual_answer = text_generator(sample_data)\n",
    "print(f\"Generated Answer: {generated_text}\")\n",
    "print(f\"Actual Answer: {actual_answer}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30823,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
