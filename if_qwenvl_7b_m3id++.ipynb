{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlDaj3EuoMpY",
        "outputId": "f4351938-1ec8-4b78-a77c-c5d8c32d491a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.0.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: qwen_vl_utils in /usr/local/lib/python3.12/dist-packages (0.0.14)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.24.2)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.24.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.10.0+cu128)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: av in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (16.1.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from qwen_vl_utils) (2.32.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->qwen_vl_utils) (2026.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: cuda-bindings==12.9.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.9.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.4.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.4.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
            "Requirement already satisfied: triton==3.6.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.0)\n",
            "Requirement already satisfied: cuda-pathfinder~=1.1 in /usr/local/lib/python3.12/dist-packages (from cuda-bindings==12.9.4->torch>=2.0.0->accelerate) (1.3.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\n",
            "Requirement already satisfied: typer>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (0.24.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub<2.0,>=1.3.0->transformers) (0.16.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: click>=8.2.1 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (8.3.1)\n",
            "Requirement already satisfied: rich>=12.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (13.9.4)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from typer>=0.24.0->typer-slim->transformers) (0.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim->transformers) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers accelerate matplotlib qwen_vl_utils datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QHdw53R4oMpa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from typing import Optional, Dict, List\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h0R7XVA33tio"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from typing import Optional\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "class M3ID_Paper:\n",
        "    \"\"\"\n",
        "    Implementation of Algorithm 1 from the paper:\n",
        "    \"Multi-Modal Hallucination Control by Visual Information Grounding\"\n",
        "\n",
        "    Optimized with KV caching: each decode step processes only the NEW token\n",
        "    instead of the full sequence, yielding ~10-50x speedup for long generations.\n",
        "\n",
        "    Main formula (Equation 4):\n",
        "    l̂* = lc + [max_k(lc)_k < log α] * ((1-αt)/αt) * (lc - lu)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Qwen2VLForConditionalGeneration,\n",
        "        processor: AutoProcessor,\n",
        "        lambda_param: float = 0.02,  # λ: forgetting rate\n",
        "        alpha: float = 0.3,           # α: confidence threshold\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.lambda_param = lambda_param\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "        # Pre-compute log(α) for indicator comparison in log-space\n",
        "        self.log_alpha = math.log(alpha)\n",
        "\n",
        "    def load_image(self, image_source) -> Image.Image:\n",
        "        \"\"\"Load image from URL or file path\"\"\"\n",
        "        if isinstance(image_source, Image.Image):\n",
        "            return image_source\n",
        "        if image_source.startswith(('http://', 'https://')):\n",
        "            response = requests.get(image_source)\n",
        "            return Image.open(BytesIO(response.content))\n",
        "        return Image.open(image_source)\n",
        "\n",
        "    def prepare_inputs_with_image(self, prompt: str, image: Image.Image):\n",
        "        \"\"\"Prepare inputs with image (conditioned)\"\"\"\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"image\", \"image\": image},\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        return self.processor(\n",
        "            text=[text], images=[image], return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "    def prepare_inputs_without_image(self, prompt: str):\n",
        "        \"\"\"Prepare inputs without image (unconditioned)\"\"\"\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\"type\": \"text\", \"text\": prompt},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        return self.processor(\n",
        "            text=[text], images=None, return_tensors=\"pt\", padding=True\n",
        "        )\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        image_path: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 0.2,\n",
        "        verbose: bool = False\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Generate text using M3ID (Algorithm 1) with KV caching.\n",
        "\n",
        "        Optimization: instead of reprocessing the full sequence at every step,\n",
        "        we cache key/value states and only feed the single new token each step.\n",
        "        This reduces per-step complexity from O(seq_len) to O(1) for attention.\n",
        "        \"\"\"\n",
        "        image = self.load_image(image_path)\n",
        "\n",
        "        # Prepare inputs\n",
        "        inputs_c = self.prepare_inputs_with_image(prompt, image)\n",
        "        inputs_u = self.prepare_inputs_without_image(prompt)\n",
        "        inputs_c = {k: v.to(self.device) for k, v in inputs_c.items()}\n",
        "        inputs_u = {k: v.to(self.device) for k, v in inputs_u.items()}\n",
        "\n",
        "        # === PREFILL PASS: process full sequences, initialize KV caches ===\n",
        "        outputs_c = self.model(**inputs_c, use_cache=True)\n",
        "        past_kv_c = outputs_c.past_key_values\n",
        "        logits_c = outputs_c.logits[:, -1, :]\n",
        "\n",
        "        outputs_u = self.model(**inputs_u, use_cache=True)\n",
        "        past_kv_u = outputs_u.past_key_values\n",
        "        logits_u = outputs_u.logits[:, -1, :]\n",
        "\n",
        "        # Track total sequence lengths for attention masks\n",
        "        seq_len_c = inputs_c['input_ids'].shape[1]\n",
        "        seq_len_u = inputs_u['input_ids'].shape[1]\n",
        "\n",
        "        # Pre-compute constants\n",
        "        inv_temp = 1.0 / temperature\n",
        "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
        "        generated_ids = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{'t':<4} {'αt':<8} {'max(pc)':<10} {'Indicator':<12} {'Token'}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        # === DECODE LOOP: each step processes only 1 token ===\n",
        "        for t in range(1, max_new_tokens + 1):\n",
        "            # Step 1: αt ← exp(-λt)\n",
        "            alpha_t = math.exp(-self.lambda_param * t)\n",
        "\n",
        "            # Step 2-3: Compute log-probs from cached logits (no re-encoding)\n",
        "            lc = torch.log_softmax(logits_c * inv_temp, dim=-1)\n",
        "            lu = torch.log_softmax(logits_u * inv_temp, dim=-1)\n",
        "\n",
        "            # Step 4: Indicator [max_k p(y_k|...) < α]\n",
        "            # Equivalent in log-space: max(log_softmax) < log(α)\n",
        "            max_log_prob_c = lc.max(dim=-1).values\n",
        "            indicator = (max_log_prob_c < self.log_alpha).float()\n",
        "\n",
        "            # Step 5: l̂* = lc + [indicator] * ((1-αt)/αt) * (lc - lu)\n",
        "            if alpha_t > 0:\n",
        "                w = (1.0 - alpha_t) / alpha_t\n",
        "                l_star = lc + indicator.unsqueeze(-1) * w * (lc - lu)\n",
        "            else:\n",
        "                l_star = lc\n",
        "\n",
        "            # Step 6: yt = argmax l̂*\n",
        "            next_token_id = l_star.argmax(dim=-1)\n",
        "\n",
        "            if next_token_id.item() == eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated_ids.append(next_token_id.item())\n",
        "\n",
        "            if verbose:\n",
        "                token_str = self.processor.tokenizer.decode([next_token_id.item()])\n",
        "                max_prob = max_log_prob_c.exp().item()\n",
        "                ind_str = \"INTERVENE\" if indicator.item() > 0 else \"NO\"\n",
        "                print(f\"{t:<4} {alpha_t:<8.4f} {max_prob:<10.4f} {ind_str:<12} {repr(token_str)}\")\n",
        "\n",
        "            # === DECODE STEP: feed only the new token with KV cache ===\n",
        "            next_token_tensor = next_token_id.view(1, 1)\n",
        "\n",
        "            seq_len_c += 1\n",
        "            attn_mask_c = torch.ones((1, seq_len_c), dtype=torch.long, device=self.device)\n",
        "            out_c = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=attn_mask_c,\n",
        "                past_key_values=past_kv_c,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits_c = out_c.logits[:, -1, :]\n",
        "            past_kv_c = out_c.past_key_values\n",
        "\n",
        "            seq_len_u += 1\n",
        "            attn_mask_u = torch.ones((1, seq_len_u), dtype=torch.long, device=self.device)\n",
        "            out_u = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=attn_mask_u,\n",
        "                past_key_values=past_kv_u,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits_u = out_u.logits[:, -1, :]\n",
        "            past_kv_u = out_u.past_key_values\n",
        "\n",
        "        return self.processor.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "\n",
        "class M3ID_WithSampling(M3ID_Paper):\n",
        "    \"\"\"\n",
        "    Extended M3ID with nucleus (top-p) sampling instead of greedy search.\n",
        "    Inherits KV-cache optimization from M3ID_Paper.\n",
        "    \"\"\"\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        image_path: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 0.2,\n",
        "        top_p: float = 0.9,\n",
        "        verbose: bool = True\n",
        "    ) -> str:\n",
        "        \"\"\"M3ID with top-p (nucleus) sampling, KV-cached.\"\"\"\n",
        "        image = self.load_image(image_path)\n",
        "\n",
        "        inputs_c = self.prepare_inputs_with_image(prompt, image)\n",
        "        inputs_u = self.prepare_inputs_without_image(prompt)\n",
        "        inputs_c = {k: v.to(self.device) for k, v in inputs_c.items()}\n",
        "        inputs_u = {k: v.to(self.device) for k, v in inputs_u.items()}\n",
        "\n",
        "        # === PREFILL ===\n",
        "        outputs_c = self.model(**inputs_c, use_cache=True)\n",
        "        past_kv_c = outputs_c.past_key_values\n",
        "        logits_c = outputs_c.logits[:, -1, :]\n",
        "\n",
        "        outputs_u = self.model(**inputs_u, use_cache=True)\n",
        "        past_kv_u = outputs_u.past_key_values\n",
        "        logits_u = outputs_u.logits[:, -1, :]\n",
        "\n",
        "        seq_len_c = inputs_c['input_ids'].shape[1]\n",
        "        seq_len_u = inputs_u['input_ids'].shape[1]\n",
        "\n",
        "        inv_temp = 1.0 / temperature\n",
        "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
        "        generated_ids = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"{'t':<4} {'αt':<8} {'max(pc)':<10} {'Indicator':<12} {'Token'}\")\n",
        "            print(\"-\" * 70)\n",
        "\n",
        "        for t in range(1, max_new_tokens + 1):\n",
        "            alpha_t = math.exp(-self.lambda_param * t)\n",
        "\n",
        "            lc = torch.log_softmax(logits_c * inv_temp, dim=-1)\n",
        "            lu = torch.log_softmax(logits_u * inv_temp, dim=-1)\n",
        "\n",
        "            max_log_prob_c = lc.max(dim=-1).values\n",
        "            indicator = (max_log_prob_c < self.log_alpha).float()\n",
        "\n",
        "            if alpha_t > 0:\n",
        "                w = (1.0 - alpha_t) / alpha_t\n",
        "                l_star = lc + indicator.unsqueeze(-1) * w * (lc - lu)\n",
        "            else:\n",
        "                l_star = lc\n",
        "\n",
        "            # Top-p sampling on adjusted log-probs\n",
        "            probs_final = torch.softmax(l_star, dim=-1)\n",
        "\n",
        "            sorted_probs, sorted_indices = torch.sort(probs_final, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            # Mask tokens beyond top-p threshold\n",
        "            sorted_mask = cumulative_probs > top_p\n",
        "            sorted_mask[..., 1:] = sorted_mask[..., :-1].clone()\n",
        "            sorted_mask[..., 0] = False\n",
        "\n",
        "            indices_to_remove = torch.zeros_like(probs_final, dtype=torch.bool)\n",
        "            indices_to_remove.scatter_(-1, sorted_indices, sorted_mask)\n",
        "            probs_final[indices_to_remove] = 0.0\n",
        "\n",
        "            # Renormalize\n",
        "            prob_sum = probs_final.sum(dim=-1, keepdim=True)\n",
        "            probs_final = torch.where(\n",
        "                prob_sum > 0,\n",
        "                probs_final / prob_sum,\n",
        "                torch.ones_like(probs_final) / probs_final.shape[-1]\n",
        "            )\n",
        "\n",
        "            next_token_id = torch.multinomial(probs_final, num_samples=1)  # [1, 1]\n",
        "\n",
        "            if next_token_id.item() == eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated_ids.append(next_token_id.item())\n",
        "\n",
        "            if verbose:\n",
        "                token_str = self.processor.tokenizer.decode([next_token_id.item()])\n",
        "                max_prob = max_log_prob_c.exp().item()\n",
        "                ind_str = \"INTERVENE\" if indicator.item() > 0 else \"NO\"\n",
        "                print(f\"{t:<4} {alpha_t:<8.4f} {max_prob:<10.4f} {ind_str:<12} {repr(token_str)}\")\n",
        "\n",
        "            # === DECODE STEP with KV cache ===\n",
        "            next_token_tensor = next_token_id.view(1, 1)\n",
        "\n",
        "            seq_len_c += 1\n",
        "            attn_mask_c = torch.ones((1, seq_len_c), dtype=torch.long, device=self.device)\n",
        "            out_c = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=attn_mask_c,\n",
        "                past_key_values=past_kv_c,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits_c = out_c.logits[:, -1, :]\n",
        "            past_kv_c = out_c.past_key_values\n",
        "\n",
        "            seq_len_u += 1\n",
        "            attn_mask_u = torch.ones((1, seq_len_u), dtype=torch.long, device=self.device)\n",
        "            out_u = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=attn_mask_u,\n",
        "                past_key_values=past_kv_u,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits_u = out_u.logits[:, -1, :]\n",
        "            past_kv_u = out_u.past_key_values\n",
        "\n",
        "        return self.processor.tokenizer.decode(generated_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "R5vTjw5aoMpc"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "login(\"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\")\n",
        "\n",
        "# import os\n",
        "# os.environ[\"HF_TOKEN\"] = \"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\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "from typing import Optional, List, Tuple\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoProcessor\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPONENT 1: Learnable γ network\n",
        "# γₜ = σ(W hₜ) — học khi nào cần trust image\n",
        "# ============================================================\n",
        "class LearnableGamma(nn.Module):\n",
        "    \"\"\"\n",
        "    Học adaptive visual trust coefficient từ hidden state.\n",
        "\n",
        "    γₜ nhỏ → model tự tin vào visual evidence → tăng correction\n",
        "    γₜ lớn → token ngữ pháp → giảm correction\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int):\n",
        "        super().__init__()\n",
        "        # Simple linear projection: W ∈ ℝ^(hidden_size → 1)\n",
        "        self.proj = nn.Linear(hidden_size, 1, bias=True)\n",
        "        # Init bias nhỏ để γ bắt đầu ~0.5\n",
        "        nn.init.zeros_(self.proj.weight)\n",
        "        nn.init.constant_(self.proj.bias, 0.0)\n",
        "\n",
        "    def forward(self, h_t: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            h_t: hidden state [batch, hidden_size]\n",
        "        Returns:\n",
        "            gamma: [batch, 1] ∈ (0, 1)\n",
        "        \"\"\"\n",
        "        return torch.sigmoid(self.proj(h_t))  # γₜ = σ(W hₜ)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# COMPONENT 2: Hybrid Patch + Region Encoder\n",
        "# Visual tokens = {patch tokens} ∪ {region tokens}\n",
        "# ============================================================\n",
        "class HybridVisualEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Kết hợp hai mức trừu tượng:\n",
        "    - Patch tokens: perception (texture, background, global context)\n",
        "    - Region tokens: grounding anchor (object-level semantic)\n",
        "\n",
        "    Trong Qwen2VL, patch tokens đã có sẵn trong visual_hidden_states.\n",
        "    Region tokens được tổng hợp từ patch tokens qua attention pooling.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size: int, num_regions: int = 16):\n",
        "        super().__init__()\n",
        "        self.num_regions = num_regions\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Learnable region query vectors (như DETR object queries)\n",
        "        # Mỗi query học cách attend vào một \"semantic region\" khác nhau\n",
        "        self.region_queries = nn.Parameter(\n",
        "            torch.randn(num_regions, hidden_size) * 0.02\n",
        "        )\n",
        "\n",
        "        # Cross-attention: region queries attend patch tokens\n",
        "        self.region_attn = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=8,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Fusion gate: học trọng số blend patch vs region\n",
        "        self.fusion_gate = nn.Linear(hidden_size * 2, 1)\n",
        "\n",
        "    def forward(self, patch_tokens: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            patch_tokens: [batch, num_patches, hidden_size]\n",
        "        Returns:\n",
        "            hybrid_tokens: [batch, num_patches + num_regions, hidden_size]\n",
        "        \"\"\"\n",
        "        batch_size = patch_tokens.shape[0]\n",
        "\n",
        "        # Expand region queries cho batch\n",
        "        queries = self.region_queries.unsqueeze(0).expand(batch_size, -1, -1)\n",
        "        # queries: [batch, num_regions, hidden_size]\n",
        "\n",
        "        # Cross-attention: region queries ← patch tokens\n",
        "        # Mỗi region query học attend vào spatial region khác nhau\n",
        "        region_tokens, _ = self.region_attn(\n",
        "            query=queries,       # [batch, num_regions, H]\n",
        "            key=patch_tokens,    # [batch, num_patches, H]\n",
        "            value=patch_tokens   # [batch, num_patches, H]\n",
        "        )\n",
        "        # region_tokens: [batch, num_regions, hidden_size]\n",
        "\n",
        "        # Concatenate: Visual tokens = patches ∪ regions\n",
        "        hybrid_tokens = torch.cat([patch_tokens, region_tokens], dim=1)\n",
        "        # [batch, num_patches + num_regions, hidden_size]\n",
        "\n",
        "        return hybrid_tokens\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# MAIN: Attention-aware M3ID với Learnable γ\n",
        "# Formula: l̂ₜ = lc + αₜ · ((1-γₜ)/γₜ) · (lc - lu)\n",
        "# ============================================================\n",
        "class HybridAttentionM3ID:\n",
        "    \"\"\"\n",
        "    Full framework:\n",
        "    1. Hybrid Patch + Region Encoder → richer visual representation\n",
        "    2. Attention-aware scaling (αₜ) → correction chỉ khi attend image\n",
        "    3. Learnable γₜ → adaptive visual trust\n",
        "\n",
        "    Decode formula:\n",
        "        l̂ₜ = lc + αₜ · ((1-γₜ)/γₜ) · (lc - lu)\n",
        "\n",
        "    So sánh với M3ID gốc:\n",
        "        l̂ₜ = lc + [indicator] · ((1-exp(-λt))/exp(-λt)) · (lc - lu)\n",
        "\n",
        "    Thay thế:\n",
        "    - exp(-λt) heuristic → γₜ = σ(W hₜ) learnable\n",
        "    - binary indicator → αₜ = attention mass (continuous)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: Qwen2_5_VLForConditionalGeneration,\n",
        "        processor: AutoProcessor,\n",
        "        hidden_size: int = 2048,       # Qwen2VL-7B hidden size is 3584 and 3B is 2048\n",
        "        num_regions: int = 16,          # số region tokens\n",
        "        gamma_lr: float = 1e-4,         # learning rate cho γ network\n",
        "        device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    ):\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "        self.device = device\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # === Learnable components ===\n",
        "        self.gamma_net = LearnableGamma(hidden_size).to(device=device, dtype=torch.float16)\n",
        "        self.hybrid_encoder = HybridVisualEncoder(hidden_size, num_regions).to(device=device, dtype=torch.float16)\n",
        "\n",
        "        # Optimizer chỉ cho các learnable components (không train lại LLM)\n",
        "        self.optimizer = torch.optim.Adam(\n",
        "            list(self.gamma_net.parameters()) +\n",
        "            list(self.hybrid_encoder.parameters()),\n",
        "            lr=gamma_lr\n",
        "        )\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Image loading\n",
        "    # ----------------------------------------------------------\n",
        "    def load_image(self, image_source) -> Image.Image:\n",
        "      if isinstance(image_source, Image.Image):\n",
        "          return image_source\n",
        "      if isinstance(image_source, str) and image_source.startswith(('http://', 'https://')):\n",
        "          headers = {\n",
        "              \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"\n",
        "          }\n",
        "          response = requests.get(image_source, headers=headers, timeout=10)\n",
        "          # Check HTTP status\n",
        "          response.raise_for_status()\n",
        "          # Check content type\n",
        "          content_type = response.headers.get(\"Content-Type\", \"\")\n",
        "          if \"image\" not in content_type:\n",
        "              raise ValueError(f\"URL did not return an image. Content-Type: {content_type}\")\n",
        "          return Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "      return Image.open(image_source).convert(\"RGB\")\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # Input preparation\n",
        "    # ----------------------------------------------------------\n",
        "    def _prepare_inputs_with_image(self, prompt: str, image: Image.Image) -> dict:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"image\", \"image\": image},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ],\n",
        "        }]\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        return self.processor(text=[text], images=[image], return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    def _prepare_inputs_without_image(self, prompt: str) -> dict:\n",
        "        messages = [{\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": prompt}],\n",
        "        }]\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        return self.processor(text=[text], images=None, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # COMPONENT: Tính attention mass αₜ\n",
        "    # αₜ = tổng attention weight từ text token hiện tại → image tokens\n",
        "    # ----------------------------------------------------------\n",
        "    def _compute_attention_mass(\n",
        "        self,\n",
        "        attentions: Tuple,              # tuple of [batch, heads, seq, seq]\n",
        "        num_image_tokens: int,\n",
        "        layer_idx: int = -1             # dùng layer cuối (most semantic)\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Tính αₜ = attention mass từ token cuối → image tokens.\n",
        "\n",
        "        Dùng layer attention cuối cùng vì nó capture\n",
        "        semantic-level dependencies (không phải syntactic).\n",
        "\n",
        "        Returns:\n",
        "            alpha_t: scalar tensor ∈ [0, 1]\n",
        "        \"\"\"\n",
        "        if attentions is None:\n",
        "            # Fallback: không có attention → assume moderate attention\n",
        "            return torch.tensor(0.5, device=self.device)\n",
        "\n",
        "        # Lọc ra các layer có attention weights thực sự (không phải None)\n",
        "        valid_attentions = [a for a in attentions if a is not None]\n",
        "\n",
        "        # Fallback 2: tất cả layers đều là None\n",
        "        if len(valid_attentions) == 0:\n",
        "            return torch.tensor(0.5, device=self.device)\n",
        "\n",
        "        # Lấy layer hợp lệ theo layer_idx\n",
        "        # Nếu layer_idx=-1 → lấy layer cuối trong danh sách hợp lệ\n",
        "        attn_layer = valid_attentions[layer_idx]  # [batch, heads, seq_len, seq_len]\n",
        "\n",
        "        # Attention từ token cuối (vị trí -1) đến tất cả positions\n",
        "        # Average across heads\n",
        "        attn_last_token = attn_layer[0, :, -1, :]   # [heads, seq_len]\n",
        "        attn_avg = attn_last_token.mean(dim=0)       # [seq_len]\n",
        "\n",
        "        # Image tokens nằm ở đầu sequence\n",
        "        image_start = 1\n",
        "        image_end = min(image_start + num_image_tokens, attn_avg.shape[0])\n",
        "\n",
        "        # αₜ = tổng attention mass đến image tokens\n",
        "        alpha_t = attn_avg[image_start:image_end].mean()\n",
        "\n",
        "        return alpha_t.clamp(0.0, 1.0)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # GENERATE: Main decode loop\n",
        "    # ----------------------------------------------------------\n",
        "    @torch.no_grad()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        image_path: str,\n",
        "        max_new_tokens: int = 100,\n",
        "        temperature: float = 0.7,\n",
        "        top_p: float = 0.9,\n",
        "        verbose: bool = True\n",
        "    ) -> str:\n",
        "        \"\"\"\n",
        "        Decode với full framework:\n",
        "\n",
        "        Mỗi bước t:\n",
        "        1. lc = log p(yₜ | x, image)\n",
        "        2. lu = log p(yₜ | x)\n",
        "        3. αₜ = attention mass → image tokens\n",
        "        4. γₜ = σ(W hₜ) từ hidden state\n",
        "        5. l̂ₜ = lc + αₜ · ((1-γₜ)/γₜ) · (lc - lu)\n",
        "        6. Sample yₜ ~ softmax(l̂ₜ)\n",
        "        \"\"\"\n",
        "        image = self.load_image(image_path)\n",
        "\n",
        "        # Prepare inputs\n",
        "        inputs_c = self._prepare_inputs_with_image(prompt, image)\n",
        "        inputs_u = self._prepare_inputs_without_image(prompt)\n",
        "        inputs_c = {k: v.to(self.device) for k, v in inputs_c.items()}\n",
        "        inputs_u = {k: v.to(self.device) for k, v in inputs_u.items()}\n",
        "\n",
        "        # === PREFILL: lần đầu chạy cả sequence, khởi tạo KV cache ===\n",
        "        # output_attentions=True để lấy αₜ\n",
        "        outputs_c = self.model(\n",
        "            **inputs_c,\n",
        "            use_cache=True,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True,   # cần hₜ cho γₜ\n",
        "        )\n",
        "        past_kv_c = outputs_c.past_key_values\n",
        "        logits_c   = outputs_c.logits[:, -1, :]\n",
        "\n",
        "        # Hidden state của layer cuối, token cuối → dùng cho γₜ\n",
        "        # hidden_states: tuple of [batch, seq, hidden], lấy layer cuối\n",
        "        h_t = outputs_c.hidden_states[-1][:, -1, :]  # [1, hidden_size]\n",
        "\n",
        "        # Số image tokens trong conditioned input\n",
        "        # Qwen2VL trả về image_grid_thw để tính số patch tokens\n",
        "        num_image_tokens = 0\n",
        "        if hasattr(outputs_c, 'image_grid_thw') or 'image_grid_thw' in inputs_c:\n",
        "            grid = inputs_c.get('image_grid_thw', None)\n",
        "            if grid is not None:\n",
        "                # num_patches = T * H * W (với T=1 cho ảnh tĩnh)\n",
        "                num_image_tokens = int(grid[0].prod().item())\n",
        "        # Fallback estimate\n",
        "        if num_image_tokens == 0:\n",
        "            num_image_tokens = 256  # typical for 448px image\n",
        "\n",
        "        # Tính αₜ từ prefill attention\n",
        "        alpha_attention = self._compute_attention_mass(\n",
        "            outputs_c.attentions,\n",
        "            num_image_tokens\n",
        "        )\n",
        "\n",
        "        # Unconditioned prefill (không cần attention/hidden)\n",
        "        outputs_u = self.model(**inputs_u, use_cache=True)\n",
        "        past_kv_u = outputs_u.past_key_values\n",
        "        logits_u  = outputs_u.logits[:, -1, :]\n",
        "\n",
        "        seq_len_c = inputs_c['input_ids'].shape[1]\n",
        "        seq_len_u = inputs_u['input_ids'].shape[1]\n",
        "\n",
        "        inv_temp = 1.0 / temperature\n",
        "        eos_token_id = self.processor.tokenizer.eos_token_id\n",
        "        generated_ids = []\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n{'t':<4} {'α_attn':<10} {'γt':<8} {'w=(1-γ)/γ':<12} {'Token'}\")\n",
        "            print(\"-\" * 60)\n",
        "\n",
        "        # === DECODE LOOP ===\n",
        "        for t in range(1, max_new_tokens + 1):\n",
        "\n",
        "            # --- Step 4: Tính γₜ từ hidden state ---\n",
        "            # Dùng gamma_net (có thể fine-tune sau)\n",
        "            # inference_mode: tạm thời enable grad chỉ cho gamma_net\n",
        "            with torch.enable_grad():\n",
        "                h_t_input = h_t.detach().to(dtype=next(self.gamma_net.parameters()).dtype)\n",
        "                gamma_t = self.gamma_net(h_t_input)  # [1, 1]\n",
        "            gamma_t = gamma_t.detach().squeeze()         # scalar\n",
        "\n",
        "            # --- Step 2-3: Log-probs ---\n",
        "            lc = torch.log_softmax(logits_c * inv_temp, dim=-1)  # [1, vocab]\n",
        "            lu = torch.log_softmax(logits_u * inv_temp, dim=-1)  # [1, vocab]\n",
        "\n",
        "            # --- Step 5: Attention-aware M3ID formula ---\n",
        "            # l̂ₜ = lc + αₜ · ((1-γₜ)/γₜ) · (lc - lu)\n",
        "            # αₜ: continuous attention mass (thay cho binary indicator)\n",
        "            # (1-γₜ)/γₜ: learnable correction weight (thay cho heuristic)\n",
        "\n",
        "            eps = 1e-6\n",
        "            correction_weight = ((1.0 - gamma_t) / (gamma_t + eps)).clamp(0.0, 5.0)\n",
        "\n",
        "            # αₜ đóng vai trò gate: chỉ correct khi thực sự attend image\n",
        "            l_star = lc + alpha_attention * correction_weight * (lc - lu)\n",
        "            # [1, vocab]\n",
        "\n",
        "            # --- Step 6: Top-p sampling ---\n",
        "            probs = torch.softmax(l_star, dim=-1)\n",
        "\n",
        "            # Nucleus sampling\n",
        "            sorted_probs, sorted_idx = torch.sort(probs, descending=True, dim=-1)\n",
        "            cum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
        "\n",
        "            mask = cum_probs > top_p\n",
        "            mask[..., 1:] = mask[..., :-1].clone()\n",
        "            mask[..., 0] = False\n",
        "\n",
        "            remove_mask = torch.zeros_like(probs, dtype=torch.bool)\n",
        "            remove_mask.scatter_(-1, sorted_idx, mask)\n",
        "            probs[remove_mask] = 0.0\n",
        "\n",
        "            prob_sum = probs.sum(dim=-1, keepdim=True).clamp(min=eps)\n",
        "            probs = probs / prob_sum\n",
        "\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1)  # [1, 1]\n",
        "\n",
        "            if next_token_id.item() == eos_token_id:\n",
        "                break\n",
        "\n",
        "            generated_ids.append(next_token_id.item())\n",
        "\n",
        "            if verbose:\n",
        "                token_str = self.processor.tokenizer.decode([next_token_id.item()])\n",
        "                w_val = correction_weight.item()\n",
        "                print(f\"{t:<4} {alpha_attention.item():<10.4f} {gamma_t.item():<8.4f} {w_val:<12.4f} {repr(token_str)}\")\n",
        "\n",
        "            # === DECODE STEP: feed 1 token mới với KV cache ===\n",
        "            next_token_tensor = next_token_id.view(1, 1)\n",
        "\n",
        "            # Conditioned: cần attention + hidden state cho bước sau\n",
        "            seq_len_c += 1\n",
        "            out_c = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=torch.ones((1, seq_len_c), dtype=torch.long, device=self.device),\n",
        "                past_key_values=past_kv_c,\n",
        "                use_cache=True,\n",
        "                output_attentions=True,\n",
        "                output_hidden_states=True,\n",
        "            )\n",
        "            logits_c  = out_c.logits[:, -1, :]\n",
        "            past_kv_c = out_c.past_key_values\n",
        "\n",
        "            # Cập nhật hₜ và αₜ cho bước tiếp theo\n",
        "            h_t = out_c.hidden_states[-1][:, -1, :]\n",
        "            alpha_attention = self._compute_attention_mass(\n",
        "                out_c.attentions,\n",
        "                num_image_tokens\n",
        "            )\n",
        "\n",
        "            # Unconditioned (không cần hidden/attention)\n",
        "            seq_len_u += 1\n",
        "            out_u = self.model(\n",
        "                input_ids=next_token_tensor,\n",
        "                attention_mask=torch.ones((1, seq_len_u), dtype=torch.long, device=self.device),\n",
        "                past_key_values=past_kv_u,\n",
        "                use_cache=True,\n",
        "            )\n",
        "            logits_u  = out_u.logits[:, -1, :]\n",
        "            past_kv_u = out_u.past_key_values\n",
        "\n",
        "        return self.processor.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    # ----------------------------------------------------------\n",
        "    # TRAINING: Fine-tune γ network + hybrid encoder\n",
        "    # Dùng khi có labeled data (response có/không hallucinate)\n",
        "    # ----------------------------------------------------------\n",
        "    def train_step(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        image_path: str,\n",
        "        target_response: str,\n",
        "        hallucination_label: float = 0.0  # 0=no hallucination, 1=hallucination\n",
        "    ) -> float:\n",
        "        \"\"\"\n",
        "        Fine-tune γ network với supervision đơn giản.\n",
        "\n",
        "        Loss: nếu hallucination_label=1 → push γ nhỏ (tăng correction)\n",
        "              nếu hallucination_label=0 → γ có thể lớn hơn\n",
        "\n",
        "        Returns:\n",
        "            loss value\n",
        "        \"\"\"\n",
        "        image = self.load_image(image_path)\n",
        "        inputs_c = self._prepare_inputs_with_image(prompt, image)\n",
        "        inputs_c = {k: v.to(self.device) for k, v in inputs_c.items()}\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                **inputs_c,\n",
        "                output_hidden_states=True,\n",
        "                use_cache=False,\n",
        "            )\n",
        "\n",
        "        # Lấy hidden states của toàn bộ generated positions\n",
        "        hidden = outputs.hidden_states[-1]  # [1, seq, hidden]\n",
        "\n",
        "        # Tính γ cho từng position\n",
        "        gammas = self.gamma_net(hidden.squeeze(0))  # [seq, 1]\n",
        "        avg_gamma = gammas.mean()\n",
        "\n",
        "        # Loss: hallucination → muốn γ nhỏ (model nên trust image nhiều hơn)\n",
        "        # no hallucination → γ tự do\n",
        "        target_gamma = torch.tensor(\n",
        "            1.0 - hallucination_label,   # hallucination → target γ = 0 (max correction)\n",
        "            device=self.device\n",
        "        )\n",
        "        loss = nn.functional.mse_loss(avg_gamma, target_gamma)\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss.item()\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# USAGE EXAMPLE\n",
        "# ============================================================\n",
        "if __name__ == \"__main__\":\n",
        "    # Load model\n",
        "    model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "    # model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "    #     model_name,\n",
        "    #     torch_dtype=torch.float16,\n",
        "    #     device_map=\"auto\",\n",
        "    # )\n",
        "\n",
        "    model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation='eager'\n",
        "    )\n",
        "\n",
        "    # Khởi tạo framework\n",
        "    framework = HybridAttentionM3ID(\n",
        "        model=model,\n",
        "        processor=processor,\n",
        "        hidden_size=2048,    # Qwen2VL-7B\n",
        "        num_regions=16,\n",
        "        # device=\"cuda\"\n",
        "    )\n",
        "\n",
        "    # Inference\n",
        "    result = framework.generate(\n",
        "        prompt=\"Describe what you see in this image in detail.\",\n",
        "        image_path=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
        "        max_new_tokens=150,\n",
        "        temperature=0.7,\n",
        "        top_p=0.9,\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    print(f\"\\n=== Generated Response ===\\n{result}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f538d361788f4d7eab96fa5801019f0e",
            "2de6eeb973874d5d9904bff2b0eb1ee4",
            "03f047cfc0e64bc2bf4401d872bff663",
            "25bf4c2314f74c2fb21d5dc877418dee",
            "e214ad5339ee4762984a2b8f8f9a74d1",
            "83f69caa5cdf4e6eb272e1b3ad721f3d",
            "ff9d8178df034c3f873129e98d84e6f9",
            "4cf40c6792624b47b6d007f26146c9c0",
            "a6c15e4751b442c6863de11877d7357e",
            "24b502f51c354374b29a4f7c0067ad1a",
            "0d8664f358ca4b688276a6faed6985fc",
            "c707d60863f34635a46f5599d92afc64",
            "078dec6062214b6c8359cc6d65d757b9",
            "96f237068b2a4cc7857682059905fc21",
            "ba23d7d828f74927bbb5fcf9c155efcb",
            "3e46a6cd512f41979d54665e4344fa86",
            "93299b0740a848bea2c9183523d3c188",
            "dd1dd35fc1004a15bdfca39042bb6eca",
            "42fe3a10f12c4e3283907c1b352e4733",
            "3a323d578ee74d9e8de6b6110e4e14a1",
            "b0781aef426946e69364355cb0ec25eb",
            "75e8201886b042d2a667e5b6efc78285",
            "cac9c6bcce654d5c837aaf510f7b9244",
            "d256dc86f40445e4831821f85b1e57ad",
            "e593908c470b48b2bf7b2e4813ac6a45",
            "2f235862b61e4a909d7c9d0bf69a7f67",
            "b80d6126743245759bb3fdd5af820073",
            "e02958a6448a4609ac13f34baf30afde",
            "f85d46fe51f847faa80af76317dfe4f7",
            "7dab5895c32a4355a38ddec52eeca1ad",
            "24c4694a950543f681a76c97e8b5e4ff",
            "6ec53ce6991a452599cf3119e4b0c22a",
            "5dcffc803517404fbb2f440e00847304"
          ]
        },
        "id": "_JWy5U_adDWf",
        "outputId": "80fdfd16-e2c6-4a18-b916-3538b2d6187c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (incomplete total...): 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f538d361788f4d7eab96fa5801019f0e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c707d60863f34635a46f5599d92afc64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/824 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cac9c6bcce654d5c837aaf510f7b9244"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "t    α_attn     γt       w=(1-γ)/γ    Token\n",
            "------------------------------------------------------------\n",
            "1    0.0005     0.5000   1.0000       'The'\n",
            "2    0.0005     0.5000   1.0000       ' image'\n",
            "3    0.0005     0.5000   1.0000       ' shows'\n",
            "4    0.0005     0.5000   1.0000       ' a'\n",
            "5    0.0005     0.5000   1.0000       ' close'\n",
            "6    0.0005     0.5000   1.0000       '-up'\n",
            "7    0.0005     0.5000   1.0000       ' of'\n",
            "8    0.0005     0.5000   1.0000       ' a'\n",
            "9    0.0005     0.5000   1.0000       ' cat'\n",
            "10   0.0005     0.5000   1.0000       \"'s\"\n",
            "11   0.0005     0.5000   1.0000       ' face'\n",
            "12   0.0005     0.5000   1.0000       '.'\n",
            "13   0.0005     0.5000   1.0000       ' The'\n",
            "14   0.0005     0.5000   1.0000       ' cat'\n",
            "15   0.0005     0.5000   1.0000       ' has'\n",
            "16   0.0005     0.5000   1.0000       ' a'\n",
            "17   0.0005     0.5000   1.0000       ' light'\n",
            "18   0.0005     0.5000   1.0000       ' orange'\n",
            "19   0.0005     0.5000   1.0000       ' or'\n",
            "20   0.0005     0.5000   1.0000       ' ginger'\n",
            "21   0.0005     0.5000   1.0000       ' coat'\n",
            "22   0.0005     0.5000   1.0000       ' with'\n",
            "23   0.0005     0.5000   1.0000       ' white'\n",
            "24   0.0005     0.5000   1.0000       ' markings'\n",
            "25   0.0005     0.5000   1.0000       ' on'\n",
            "26   0.0005     0.5000   1.0000       ' its'\n",
            "27   0.0005     0.5000   1.0000       ' chest'\n",
            "28   0.0005     0.5000   1.0000       ' and'\n",
            "29   0.0005     0.5000   1.0000       ' chin'\n",
            "30   0.0005     0.5000   1.0000       '.'\n",
            "31   0.0005     0.5000   1.0000       ' Its'\n",
            "32   0.0005     0.5000   1.0000       ' fur'\n",
            "33   0.0005     0.5000   1.0000       ' appears'\n",
            "34   0.0005     0.5000   1.0000       ' soft'\n",
            "35   0.0005     0.5000   1.0000       ' and'\n",
            "36   0.0005     0.5000   1.0000       ' well'\n",
            "37   0.0005     0.5000   1.0000       '-g'\n",
            "38   0.0005     0.5000   1.0000       'room'\n",
            "39   0.0005     0.5000   1.0000       'ed'\n",
            "40   0.0005     0.5000   1.0000       '.'\n",
            "41   0.0005     0.5000   1.0000       ' The'\n",
            "42   0.0005     0.5000   1.0000       ' cat'\n",
            "43   0.0005     0.5000   1.0000       \"'s\"\n",
            "44   0.0005     0.5000   1.0000       ' eyes'\n",
            "45   0.0005     0.5000   1.0000       ' are'\n",
            "46   0.0005     0.5000   1.0000       ' open'\n",
            "47   0.0005     0.5000   1.0000       ' and'\n",
            "48   0.0005     0.5000   1.0000       ' looking'\n",
            "49   0.0005     0.5000   1.0000       ' slightly'\n",
            "50   0.0005     0.5000   1.0000       ' to'\n",
            "51   0.0005     0.5000   1.0000       ' the'\n",
            "52   0.0005     0.5000   1.0000       ' side'\n",
            "53   0.0005     0.5000   1.0000       ','\n",
            "54   0.0005     0.5000   1.0000       ' with'\n",
            "55   0.0005     0.5000   1.0000       ' a'\n",
            "56   0.0005     0.5000   1.0000       ' relaxed'\n",
            "57   0.0005     0.5000   1.0000       ' expression'\n",
            "58   0.0005     0.5000   1.0000       '.'\n",
            "59   0.0005     0.5000   1.0000       ' The'\n",
            "60   0.0005     0.5000   1.0000       ' background'\n",
            "61   0.0005     0.5000   1.0000       ' is'\n",
            "62   0.0005     0.5000   1.0000       ' out'\n",
            "63   0.0005     0.5000   1.0000       ' of'\n",
            "64   0.0005     0.5000   1.0000       ' focus'\n",
            "65   0.0005     0.5000   1.0000       ','\n",
            "66   0.0005     0.5000   1.0000       ' but'\n",
            "67   0.0005     0.5000   1.0000       ' it'\n",
            "68   0.0005     0.5000   1.0000       ' seems'\n",
            "69   0.0005     0.5000   1.0000       ' to'\n",
            "70   0.0005     0.5000   1.0000       ' be'\n",
            "71   0.0005     0.5000   1.0000       ' an'\n",
            "72   0.0005     0.5000   1.0000       ' outdoor'\n",
            "73   0.0005     0.5000   1.0000       ' setting'\n",
            "74   0.0005     0.5000   1.0000       ' with'\n",
            "75   0.0005     0.5000   1.0000       ' some'\n",
            "76   0.0005     0.5000   1.0000       ' red'\n",
            "77   0.0005     0.5000   1.0000       ' objects'\n",
            "78   0.0005     0.5000   1.0000       ' in'\n",
            "79   0.0005     0.5000   1.0000       ' the'\n",
            "80   0.0005     0.5000   1.0000       ' distance'\n",
            "81   0.0005     0.5000   1.0000       '.'\n",
            "\n",
            "=== Generated Response ===\n",
            "The image shows a close-up of a cat's face. The cat has a light orange or ginger coat with white markings on its chest and chin. Its fur appears soft and well-groomed. The cat's eyes are open and looking slightly to the side, with a relaxed expression. The background is out of focus, but it seems to be an outdoor setting with some red objects in the distance.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # ============================================================\n",
        "# # USAGE EXAMPLE\n",
        "# # ============================================================\n",
        "# if __name__ == \"__main__\":\n",
        "#     # Load model\n",
        "#     model_name = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
        "\n",
        "#     processor = AutoProcessor.from_pretrained(model_name)\n",
        "#     # model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "#     #     model_name,\n",
        "#     #     torch_dtype=torch.float16,\n",
        "#     #     device_map=\"auto\",\n",
        "#     # )\n",
        "\n",
        "#     model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "#         model_name,\n",
        "#         torch_dtype=torch.float16,\n",
        "#         device_map=\"auto\",\n",
        "#     )\n",
        "\n",
        "#     # Khởi tạo framework\n",
        "#     framework = HybridAttentionM3ID(\n",
        "#         model=model,\n",
        "#         processor=processor,\n",
        "#         hidden_size=3584,    # Qwen2VL-7B\n",
        "#         num_regions=16,\n",
        "#         device=\"cuda\"\n",
        "#     )\n",
        "\n",
        "#     # Inference\n",
        "#     result = framework.generate(\n",
        "#         prompt=\"Describe what you see in this image in detail.\",\n",
        "#         image_path=\"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/1200px-Cat03.jpg\",\n",
        "#         max_new_tokens=150,\n",
        "#         temperature=0.7,\n",
        "#         top_p=0.9,\n",
        "#         verbose=True\n",
        "#     )\n",
        "\n",
        "#     print(f\"\\n=== Generated Response ===\\n{result}\")"
      ],
      "metadata": {
        "id": "fIy6HwX_epFX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ```\n",
        "\n",
        "# ---\n",
        "\n",
        "# ## Giải thích các thay đổi chính\n",
        "\n",
        "# ### 1. γₜ: Từ heuristic → Learnable\n",
        "# ```\n",
        "# Cũ:  gamma_t = exp(-λt)          # chỉ phụ thuộc thời gian\n",
        "# Mới: gamma_t = σ(W · h_t)        # phụ thuộc nội dung token\n",
        "# ```\n",
        "# `LearnableGamma` là một linear layer + sigmoid — nhỏ nhưng học được \"khi nào cần trust image\". Để train nó bạn dùng `train_step()`.\n",
        "\n",
        "# ### 2. αₜ: Từ binary indicator → Continuous attention mass\n",
        "# ```\n",
        "# Cũ:  indicator = [max(lc) < log(α)]    # binary 0/1\n",
        "# Mới: alpha_t = sum(attn → image tokens) # continuous [0,1]\n",
        "# ```\n",
        "# `_compute_attention_mass` đọc attention weights của layer cuối, tính tổng mass đến image tokens. Token như \"the\" sẽ có αₜ ≈ 0, token như \"dog\" sẽ có αₜ cao.\n",
        "\n",
        "# ### 3. Formula tổng hợp\n",
        "# ```\n",
        "# Cũ:  l̂ = lc + [ind] · ((1 - exp(-λt)) / exp(-λt)) · (lc - lu)\n",
        "# Mới: l̂ = lc + α_attn · ((1 - γₜ) / γₜ) · (lc - lu)"
      ],
      "metadata": {
        "id": "zJgx2qf3dG1r"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CprCZ_-L_4Hf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 857191,
          "sourceId": 1462296,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 9364369,
          "sourceId": 14658640,
          "sourceType": "datasetVersion"
        }
      ],
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f538d361788f4d7eab96fa5801019f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2de6eeb973874d5d9904bff2b0eb1ee4",
              "IPY_MODEL_03f047cfc0e64bc2bf4401d872bff663",
              "IPY_MODEL_25bf4c2314f74c2fb21d5dc877418dee"
            ],
            "layout": "IPY_MODEL_e214ad5339ee4762984a2b8f8f9a74d1"
          }
        },
        "2de6eeb973874d5d9904bff2b0eb1ee4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83f69caa5cdf4e6eb272e1b3ad721f3d",
            "placeholder": "​",
            "style": "IPY_MODEL_ff9d8178df034c3f873129e98d84e6f9",
            "value": "Download complete: "
          }
        },
        "03f047cfc0e64bc2bf4401d872bff663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4cf40c6792624b47b6d007f26146c9c0",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a6c15e4751b442c6863de11877d7357e",
            "value": 0
          }
        },
        "25bf4c2314f74c2fb21d5dc877418dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24b502f51c354374b29a4f7c0067ad1a",
            "placeholder": "​",
            "style": "IPY_MODEL_0d8664f358ca4b688276a6faed6985fc",
            "value": " 0.00/0.00 [00:00&lt;?, ?B/s]"
          }
        },
        "e214ad5339ee4762984a2b8f8f9a74d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f69caa5cdf4e6eb272e1b3ad721f3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9d8178df034c3f873129e98d84e6f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cf40c6792624b47b6d007f26146c9c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "a6c15e4751b442c6863de11877d7357e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "24b502f51c354374b29a4f7c0067ad1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d8664f358ca4b688276a6faed6985fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c707d60863f34635a46f5599d92afc64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_078dec6062214b6c8359cc6d65d757b9",
              "IPY_MODEL_96f237068b2a4cc7857682059905fc21",
              "IPY_MODEL_ba23d7d828f74927bbb5fcf9c155efcb"
            ],
            "layout": "IPY_MODEL_3e46a6cd512f41979d54665e4344fa86"
          }
        },
        "078dec6062214b6c8359cc6d65d757b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93299b0740a848bea2c9183523d3c188",
            "placeholder": "​",
            "style": "IPY_MODEL_dd1dd35fc1004a15bdfca39042bb6eca",
            "value": "Fetching 2 files: 100%"
          }
        },
        "96f237068b2a4cc7857682059905fc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42fe3a10f12c4e3283907c1b352e4733",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a323d578ee74d9e8de6b6110e4e14a1",
            "value": 2
          }
        },
        "ba23d7d828f74927bbb5fcf9c155efcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0781aef426946e69364355cb0ec25eb",
            "placeholder": "​",
            "style": "IPY_MODEL_75e8201886b042d2a667e5b6efc78285",
            "value": " 2/2 [00:00&lt;00:00, 82.61it/s]"
          }
        },
        "3e46a6cd512f41979d54665e4344fa86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93299b0740a848bea2c9183523d3c188": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd1dd35fc1004a15bdfca39042bb6eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42fe3a10f12c4e3283907c1b352e4733": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a323d578ee74d9e8de6b6110e4e14a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0781aef426946e69364355cb0ec25eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e8201886b042d2a667e5b6efc78285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac9c6bcce654d5c837aaf510f7b9244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d256dc86f40445e4831821f85b1e57ad",
              "IPY_MODEL_e593908c470b48b2bf7b2e4813ac6a45",
              "IPY_MODEL_2f235862b61e4a909d7c9d0bf69a7f67"
            ],
            "layout": "IPY_MODEL_b80d6126743245759bb3fdd5af820073"
          }
        },
        "d256dc86f40445e4831821f85b1e57ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e02958a6448a4609ac13f34baf30afde",
            "placeholder": "​",
            "style": "IPY_MODEL_f85d46fe51f847faa80af76317dfe4f7",
            "value": "Loading weights: 100%"
          }
        },
        "e593908c470b48b2bf7b2e4813ac6a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7dab5895c32a4355a38ddec52eeca1ad",
            "max": 824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24c4694a950543f681a76c97e8b5e4ff",
            "value": 824
          }
        },
        "2f235862b61e4a909d7c9d0bf69a7f67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec53ce6991a452599cf3119e4b0c22a",
            "placeholder": "​",
            "style": "IPY_MODEL_5dcffc803517404fbb2f440e00847304",
            "value": " 824/824 [00:35&lt;00:00, 45.68it/s, Materializing param=model.visual.patch_embed.proj.weight]"
          }
        },
        "b80d6126743245759bb3fdd5af820073": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02958a6448a4609ac13f34baf30afde": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f85d46fe51f847faa80af76317dfe4f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7dab5895c32a4355a38ddec52eeca1ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24c4694a950543f681a76c97e8b5e4ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ec53ce6991a452599cf3119e4b0c22a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcffc803517404fbb2f440e00847304": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}