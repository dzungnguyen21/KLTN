{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":39911,"sourceType":"datasetVersion","datasetId":31296},{"sourceId":2598787,"sourceType":"datasetVersion","datasetId":1573501},{"sourceId":13915433,"sourceType":"datasetVersion","datasetId":8866651}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# import os\n# !wget https://github.com/RUCAIBox/POPE/blob/main/POPEv2/dataset/annotations.json -P /kaggle/working/data/annotations/pope","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pycocotools.coco import COCO\n\ncoco = COCO(\"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/annotations/instances_train2014.json\")\n\nimg_id = 123\nann_ids = coco.getAnnIds(imgIds=img_id)\nanns = coco.loadAnns(ann_ids)\n\nobjects = set()\nfor ann in anns:\n    cat = coco.loadCats(ann[\"category_id\"])[0][\"name\"]\n    objects.add(cat)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\nfrom tqdm import tqdm\n\nLLAVA_JSON = \"/kaggle/input/llava-instruct-150k/llava_instruct_150k.json\"\n\nCOCO_DIRS = [\n    \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/train2014\",\n    \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/val2014\",\n    \"/kaggle/input/coco-2014-dataset-for-yolov3/coco2014/images/test2014\",\n]\n\nOUTPUT_JSONL = \"/kaggle/working/llava_train.jsonl\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def find_img(img_name):\n    coco_fix = [\n        \"COCO_train2014_\",\n        \"COCO_val2014\",\n        \"COCO_tes2014\",\n    ]\n\n    for coco_dir in COCO_DIRS:\n        for prefix in coco_fix:\n            path = os.path.join(coco_dir, prefix + img_name)\n            if os.path.exists(path):\n                return path\n    return None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open(LLAVA_JSON, \"r\") as f:\n    llava_data = json.load(f)\n\nvalid = 0\nmissing = 0\n\nwith open(OUTPUT_JSONL, \"w\") as out:\n    for sample in tqdm(llava_data):\n        image_name = sample.get(\"image\", \"\")\n        image_path = find_img(image_name)\n\n        if image_path is None:\n            missing += 1\n            continue\n\n        record = {\n            \"id\": sample.get(\"id\"),\n            \"image\": image_path,\n            \"conversations\": sample[\"conversations\"]\n        }\n\n        out.write(json.dumps(record) + \"\\n\")\n        valid += 1\n\nprint(f\"Valid samples: {valid}\")\nprint(f\"Missing images: {missing}\")\nprint(f\"Saved to: {OUTPUT_JSONL}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import random\n\nwith open(OUTPUT_JSONL) as f:\n    lines = f.readlines()\n\nsample = json.loads(random.choice(lines))\nprint(sample[\"image\"])\nprint(sample[\"conversations\"][0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# !pip install --upgrade transformers accelerate bitsandbytes sentencepiece protobuf pillow torch","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import (\n    CLIPVisionModel, CLIPImageProcessor,\n    LlamaForCausalLM, LlamaTokenizer,\n    get_linear_schedule_with_warmup\n)\nfrom PIL import Image\nimport json\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport os\n\n# Set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/dzungnguyen21/VLM.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd VLM","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!dir","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install uv","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!uv pip install -r requirements.txt","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python train.py --train_data=\"/kaggle/working/llava_train.jsonl\" --load_in_8bit --limit_sample 10000 --output_dir \"/kaggle/working/weights\" --epochs 5","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}